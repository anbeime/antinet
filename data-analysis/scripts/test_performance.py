"""
æ€§èƒ½æµ‹è¯•è„šæœ¬
æµ‹è¯•NPUæ¨ç†å»¶è¿Ÿå’Œå‘é‡æ£€ç´¢å“åº”æ—¶é—´
"""
import time
import sys
import os
from typing import Dict, List

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))


class PerformanceTest:
    """æ€§èƒ½æµ‹è¯•ç±»"""
    
    def __init__(self):
        self.results = {}
    
    def test_npu_inference(self, test_text: str = "åˆ†æé”€å”®è¶‹åŠ¿", 
                          iterations: int = 10) -> Dict:
        """
        æµ‹è¯•NPUæ¨ç†æ€§èƒ½
        
        å‚æ•°ï¼š
            test_text: æµ‹è¯•æ–‡æœ¬
            iterations: æµ‹è¯•æ¬¡æ•°
        
        è¿”å›ï¼š
            æ€§èƒ½æŒ‡æ ‡
        """
        print("\n" + "=" * 80)
        print("NPUæ¨ç†æ€§èƒ½æµ‹è¯•")
        print("=" * 80)
        
        latencies = []

        # å¯¼å…¥çœŸå®çš„ NPU æ¨¡å‹åŠ è½½å™¨
        import sys
        from pathlib import Path
        backend_path = Path(__file__).parent.parent.parent / "backend"
        if str(backend_path) not in sys.path:
            sys.path.insert(0, str(backend_path))

        try:
            from models.model_loader import get_model_loader
            loader = get_model_loader()
            model = loader.load()
        except Exception as e:
            raise RuntimeError(f"NPUæ¨¡å‹åŠ è½½å¤±è´¥ï¼Œæ— æ³•è¿›è¡Œæ€§èƒ½æµ‹è¯•: {e}") from e

        for i in range(iterations):
            start_time = time.time()

            # çœŸå® NPU æ¨ç†
            result = loader.infer(test_text, max_new_tokens=500, temperature=0.7)

            end_time = time.time()
            latency_ms = (end_time - start_time) * 1000
            latencies.append(latency_ms)
            
            print(f"  ç¬¬{i+1}æ¬¡æ¨ç†: {latency_ms:.2f}ms")
        
        # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
        avg_latency = sum(latencies) / len(latencies)
        min_latency = min(latencies)
        max_latency = max(latencies)
        p95_latency = sorted(latencies)[int(len(latencies) * 0.95)]
        p99_latency = sorted(latencies)[int(len(latencies) * 0.99)]
        
        result = {
            "avg_latency_ms": avg_latency,
            "min_latency_ms": min_latency,
            "max_latency_ms": max_latency,
            "p95_latency_ms": p95_latency,
            "p99_latency_ms": p99_latency,
            "iterations": iterations,
            "target_latency_ms": 500
        }
        
        # æ‰“å°ç»“æœ
        print(f"\nNPUæ¨ç†æ€§èƒ½æŒ‡æ ‡ï¼š")
        print(f"  å¹³å‡å»¶è¿Ÿ: {avg_latency:.2f}ms (ç›®æ ‡: <500ms)")
        print(f"  æœ€å°å»¶è¿Ÿ: {min_latency:.2f}ms")
        print(f"  æœ€å¤§å»¶è¿Ÿ: {max_latency:.2f}ms")
        print(f"  P95å»¶è¿Ÿ: {p95_latency:.2f}ms")
        print(f"  P99å»¶è¿Ÿ: {p99_latency:.2f}ms")
        print(f"  æµ‹è¯•æ¬¡æ•°: {iterations}")
        
        # åˆ¤æ–­æ˜¯å¦è¾¾æ ‡
        if avg_latency < 500:
            print(f"  è¾¾æ ‡ï¼šå¹³å‡å»¶è¿Ÿ {avg_latency:.2f}ms < 500ms")
        else:
            print(f" no æœªè¾¾æ ‡ï¼šå¹³å‡å»¶è¿Ÿ {avg_latency:.2f}ms > 500ms")
        
        self.results["npu_inference"] = result
        return result
    
    def test_vector_retrieval(self, queries: List[str] = None,
                             top_k: int = 10, iterations: int = 100) -> Dict:
        """
        æµ‹è¯•å‘é‡æ£€ç´¢æ€§èƒ½
        
        å‚æ•°ï¼š
            queries: æŸ¥è¯¢åˆ—è¡¨
            top_k: è¿”å›Top-Kç»“æœ
            iterations: æµ‹è¯•æ¬¡æ•°
        
        è¿”å›ï¼š
            æ€§èƒ½æŒ‡æ ‡
        """
        print("\n" + "=" * 80)
        print("å‘é‡æ£€ç´¢æ€§èƒ½æµ‹è¯•")
        print("=" * 80)
        
        # å°è¯•å¯¼å…¥å‘é‡æ£€ç´¢æ¨¡å—
        try:
            from scripts.vector_retrieval import VectorRetrieval
            
            # åˆ›å»ºç´¢å¼•å¹¶æ·»åŠ æµ‹è¯•æ•°æ®
            print("æ­£åœ¨åˆå§‹åŒ–å‘é‡æ£€ç´¢...")
            vr = VectorRetrieval()
            
            # æ·»åŠ æµ‹è¯•æ–‡æ¡£
            test_docs = [
                {"id": f"doc_{i}", "text": f"æµ‹è¯•æ–‡æ¡£{i}ï¼ŒåŒ…å«é”€å”®æ•°æ®å’Œé£é™©åˆ†æ", 
                 "metadata": {"type": "test"}}
                for i in range(1000)
            ]
            vr.add_documents(test_docs)
            
            # å‡†å¤‡æŸ¥è¯¢
            if queries is None:
                queries = [
                    "é”€å”®æ•°æ®ç»Ÿè®¡",
                    "é£é™©åˆ†æ",
                    "åº“å­˜ç®¡ç†",
                    "å®¢æˆ·åˆ†æ",
                    "å¸‚åœºè¶‹åŠ¿"
                ]
            
            # æ€§èƒ½æµ‹è¯•
            latencies = []
            result_counts = []
            
            for i in range(iterations):
                query = queries[i % len(queries)]
                start_time = time.time()
                
                results = vr.search(query, top_k=top_k)
                
                end_time = time.time()
                latency_ms = (end_time - start_time) * 1000
                latencies.append(latency_ms)
                result_counts.append(len(results))
            
            # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
            avg_latency = sum(latencies) / len(latencies)
            min_latency = min(latencies)
            max_latency = max(latencies)
            p95_latency = sorted(latencies)[int(len(latencies) * 0.95)]
            p99_latency = sorted(latencies)[int(len(latencies) * 0.99)]
            avg_result_count = sum(result_counts) / len(result_counts)
            
            result = {
                "avg_latency_ms": avg_latency,
                "min_latency_ms": min_latency,
                "max_latency_ms": max_latency,
                "p95_latency_ms": p95_latency,
                "p99_latency_ms": p99_latency,
                "avg_result_count": avg_result_count,
                "top_k": top_k,
                "iterations": iterations,
                "target_latency_ms": 100,
                "document_count": len(test_docs)
            }
            
            # æ‰“å°ç»“æœ
            print(f"\nå‘é‡æ£€ç´¢æ€§èƒ½æŒ‡æ ‡ï¼š")
            print(f"  å¹³å‡å»¶è¿Ÿ: {avg_latency:.2f}ms (ç›®æ ‡: <100ms)")
            print(f"  æœ€å°å»¶è¿Ÿ: {min_latency:.2f}ms")
            print(f"  æœ€å¤§å»¶è¿Ÿ: {max_latency:.2f}ms")
            print(f"  P95å»¶è¿Ÿ: {p95_latency:.2f}ms")
            print(f"  P99å»¶è¿Ÿ: {p99_latency:.2f}ms")
            print(f"  å¹³å‡è¿”å›ç»“æœæ•°: {avg_result_count:.2f}")
            print(f"  æµ‹è¯•æ¬¡æ•°: {iterations}")
            print(f"  æ–‡æ¡£æ•°é‡: {len(test_docs)}")
            
            # åˆ¤æ–­æ˜¯å¦è¾¾æ ‡
            if avg_latency < 100:
                print(f"  è¾¾æ ‡ï¼šå¹³å‡å»¶è¿Ÿ {avg_latency:.2f}ms < 100ms")
            else:
                print(f" no æœªè¾¾æ ‡ï¼šå¹³å‡å»¶è¿Ÿ {avg_latency:.2f}ms > 100ms")
            
            self.results["vector_retrieval"] = result
            return result
            
        except ImportError as e:
            print(f"[WARN] å‘é‡æ£€ç´¢æ¨¡å—æœªå®‰è£…ï¼Œè·³è¿‡æµ‹è¯•: {e}")
            result = {
                "error": "å‘é‡æ£€ç´¢æ¨¡å—æœªå®‰è£…",
                "avg_latency_ms": 0,
                "target_latency_ms": 100
            }
            self.results["vector_retrieval"] = result
            return result
    
    def test_batch_processing(self, file_count: int = 100,
                            avg_file_size_kb: int = 100) -> Dict:
        """
        æµ‹è¯•æ‰¹å¤„ç†æ€§èƒ½
        
        å‚æ•°ï¼š
            file_count: æ–‡ä»¶æ•°é‡
            avg_file_size_kb: å¹³å‡æ–‡ä»¶å¤§å°ï¼ˆKBï¼‰
        
        è¿”å›ï¼š
            æ€§èƒ½æŒ‡æ ‡
        """
        print("\n" + "=" * 80)
        print("æ‰¹å¤„ç†æ€§èƒ½æµ‹è¯•")
        print("=" * 80)
        
        start_time = time.time()

        # çœŸå®æ‰¹å¤„ç†
        from pathlib import Path
        temp_dir = Path(__file__).parent / "temp_test_files"
        temp_dir.mkdir(exist_ok=True)

        # åˆ›å»ºæµ‹è¯•æ–‡ä»¶
        test_files = []
        for i in range(file_count):
            test_file = temp_dir / f"test_{i}.txt"
            test_file.write_text("x" * (avg_file_size_kb * 1024))
            test_files.append(test_file)

        try:
            # å¯¼å…¥æ‰¹å¤„ç†å™¨
            from batch_process import batch_process

            # æ‰§è¡Œæ‰¹å¤„ç†
            result = batch_process(str(temp_dir))
            processed_files = len(result)
        except Exception as e:
            raise RuntimeError(f"æ‰¹å¤„ç†å¤±è´¥: {e}") from e
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            for test_file in test_files:
                if test_file.exists():
                    test_file.unlink()
            if temp_dir.exists():
                temp_dir.rmdir()

        total_size_kb = file_count * avg_file_size_kb
        end_time = time.time()
        total_time = end_time - start_time
        throughput = file_count / total_time if total_time > 0 else 0
        throughput_mb = (total_size_kb / 1024) / total_time if total_time > 0 else 0
        
        result = {
            "file_count": file_count,
            "total_size_mb": total_size_kb / 1024,
            "total_time_s": total_time,
            "throughput_files_per_min": throughput * 60,
            "throughput_mb_per_min": throughput_mb * 60,
            "target_throughput_files_per_min": 1000
        }
        
        # æ‰“å°ç»“æœ
        print(f"\næ‰¹å¤„ç†æ€§èƒ½æŒ‡æ ‡ï¼š")
        print(f"  æ–‡ä»¶æ•°é‡: {file_count}")
        print(f"  æ€»å¤§å°: {total_size_kb / 1024:.2f}MB")
        print(f"  æ€»è€—æ—¶: {total_time:.2f}ç§’")
        print(f"  ååé‡: {throughput * 60:.0f}ä¸ª/åˆ†é’Ÿ (ç›®æ ‡: >1000ä¸ª/åˆ†é’Ÿ)")
        print(f"  æ•°æ®åå: {throughput_mb * 60:.2f}MB/åˆ†é’Ÿ")
        
        # åˆ¤æ–­æ˜¯å¦è¾¾æ ‡
        if throughput * 60 >= 1000:
            print(f"  è¾¾æ ‡ï¼šååé‡ {throughput * 60:.0f}ä¸ª/åˆ†é’Ÿ >= 1000ä¸ª/åˆ†é’Ÿ")
        else:
            print(f" no æœªè¾¾æ ‡ï¼šååé‡ {throughput * 60:.0f}ä¸ª/åˆ†é’Ÿ < 1000ä¸ª/åˆ†é’Ÿ")
        
        self.results["batch_processing"] = result
        return result
    
    def test_ocr_performance(self, image_count: int = 50) -> Dict:
        """
        æµ‹è¯•OCRæ€§èƒ½
        
        å‚æ•°ï¼š
            image_count: å›¾åƒæ•°é‡
        
        è¿”å›ï¼š
            æ€§èƒ½æŒ‡æ ‡
        """
        print("\n" + "=" * 80)
        print("OCRæ€§èƒ½æµ‹è¯•")
        print("=" * 80)
        
        start_time = time.time()

        # çœŸå® OCR å¤„ç†
        try:
            import pytesseract
            from PIL import Image

            # å¯¼å…¥ NPU OCR æ¨¡å—
            import sys
            from pathlib import Path
            backend_path = Path(__file__).parent.parent.parent / "backend"
            if str(backend_path) not in sys.path:
                sys.path.insert(0, str(backend_path))

            # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ NPU OCR
            try:
                from npu_core import NPUInferenceCore
                use_npu_ocr = True
            except ImportError:
                use_npu_ocr = False

            # åˆ›å»ºæµ‹è¯•å›¾åƒ
            from PIL import Image, ImageDraw, ImageFont
            temp_dir = Path(__file__).parent / "temp_test_images"
            temp_dir.mkdir(exist_ok=True)
            test_images = []

            try:
                for i in range(image_count):
                    img = Image.new('RGB', (100, 100), color='white')
                    draw = ImageDraw.Draw(img)
                    draw.text((10, 10), f"æµ‹è¯•{i}", fill='black')
                    img_path = temp_dir / f"test_{i}.png"
                    img.save(img_path)
                    test_images.append(img_path)

                # æ‰§è¡Œ OCR
                for img_path in test_images:
                    image = Image.open(img_path)
                    text = pytesseract.image_to_string(image, lang='chi_sim+eng')

            finally:
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                for img_path in test_images:
                    if img_path.exists():
                        img_path.unlink()
                if temp_dir.exists():
                    temp_dir.rmdir()

        except ImportError as e:
            raise RuntimeError(f"OCR åº“æœªå®‰è£…ï¼Œæ— æ³•è¿›è¡Œæ€§èƒ½æµ‹è¯•: {e}") from e
        except Exception as e:
            raise RuntimeError(f"OCR å¤„ç†å¤±è´¥: {e}") from e

        end_time = time.time()
        total_time = end_time - start_time
        avg_time_per_image = total_time / image_count if image_count > 0 else 0
        
        result = {
            "image_count": image_count,
            "total_time_s": total_time,
            "avg_time_per_image_ms": avg_time_per_image * 1000,
            "throughput_images_per_min": image_count / total_time * 60 if total_time > 0 else 0,
            "target_avg_time_per_image_ms": 500
        }
        
        # æ‰“å°ç»“æœ
        print(f"\nOCRæ€§èƒ½æŒ‡æ ‡ï¼š")
        print(f"  å›¾åƒæ•°é‡: {image_count}")
        print(f"  æ€»è€—æ—¶: {total_time:.2f}ç§’")
        print(f"  å¹³å‡å¤„ç†æ—¶é—´: {avg_time_per_image * 1000:.2f}ms/å¼  (ç›®æ ‡: <500ms)")
        print(f"  ååé‡: {image_count / total_time * 60:.0f}å¼ /åˆ†é’Ÿ")
        
        # åˆ¤æ–­æ˜¯å¦è¾¾æ ‡
        if avg_time_per_image * 1000 < 500:
            print(f"  è¾¾æ ‡ï¼šå¹³å‡å¤„ç†æ—¶é—´ {avg_time_per_image * 1000:.2f}ms < 500ms")
        else:
            print(f" no æœªè¾¾æ ‡ï¼šå¹³å‡å¤„ç†æ—¶é—´ {avg_time_per_image * 1000:.2f}ms > 500ms")
        
        self.results["ocr_performance"] = result
        return result
    
    def generate_report(self) -> str:
        """
        ç”Ÿæˆæ€§èƒ½æµ‹è¯•æŠ¥å‘Š
        
        è¿”å›ï¼š
            æŠ¥å‘Šæ–‡æœ¬
        """
        report = "=" * 80 + "\n"
        report += "æ€§èƒ½æµ‹è¯•æŠ¥å‘Š\n"
        report += "=" * 80 + "\n\n"
        
        # NPUæ¨ç†
        if "npu_inference" in self.results:
            npu = self.results["npu_inference"]
            report += "1. NPUæ¨ç†æ€§èƒ½\n"
            report += "-" * 40 + "\n"
            report += f"  å¹³å‡å»¶è¿Ÿ: {npu['avg_latency_ms']:.2f}ms (ç›®æ ‡: <{npu['target_latency_ms']}ms) "
            report += "è¾¾æ ‡\n" if npu['avg_latency_ms'] < npu['target_latency_ms'] else " æœªè¾¾æ ‡\n"
            report += f"  P95å»¶è¿Ÿ: {npu['p95_latency_ms']:.2f}ms\n"
            report += f"  P99å»¶è¿Ÿ: {npu['p99_latency_ms']:.2f}ms\n\n"
        
        # å‘é‡æ£€ç´¢
        if "vector_retrieval" in self.results:
            vr = self.results["vector_retrieval"]
            if "error" not in vr:
                report += "2. å‘é‡æ£€ç´¢æ€§èƒ½\n"
                report += "-" * 40 + "\n"
                report += f"  å¹³å‡å»¶è¿Ÿ: {vr['avg_latency_ms']:.2f}ms (ç›®æ ‡: <{vr['target_latency_ms']}ms) "
                report += "è¾¾æ ‡\n" if vr['avg_latency_ms'] < vr['target_latency_ms'] else " æœªè¾¾æ ‡\n"
                report += f"  æ–‡æ¡£æ•°é‡: {vr['document_count']}\n"
                report += f"  å¹³å‡è¿”å›ç»“æœæ•°: {vr['avg_result_count']:.2f}\n\n"
        
        # æ‰¹å¤„ç†
        if "batch_processing" in self.results:
            bp = self.results["batch_processing"]
            report += "3. æ‰¹å¤„ç†æ€§èƒ½\n"
            report += "-" * 40 + "\n"
            report += f"  ååé‡: {bp['throughput_files_per_min']:.0f}ä¸ª/åˆ†é’Ÿ (ç›®æ ‡: >{bp['target_throughput_files_per_min']}ä¸ª/åˆ†é’Ÿ) "
            report += "è¾¾æ ‡\n" if bp['throughput_files_per_min'] >= bp['target_throughput_files_per_min'] else " æœªè¾¾æ ‡\n"
            report += f"  æ€»å¤§å°: {bp['total_size_mb']:.2f}MB\n\n"
        
        # OCRæ€§èƒ½
        if "ocr_performance" in self.results:
            ocr = self.results["ocr_performance"]
            report += "4. OCRæ€§èƒ½\n"
            report += "-" * 40 + "\n"
            report += f"  å¹³å‡å¤„ç†æ—¶é—´: {ocr['avg_time_per_image_ms']:.2f}ms/å¼  (ç›®æ ‡: <{ocr['target_avg_time_per_image_ms']}ms) "
            report += "è¾¾æ ‡\n" if ocr['avg_time_per_image_ms'] < ocr['target_avg_time_per_image_ms'] else " æœªè¾¾æ ‡\n"
            report += f"  ååé‡: {ocr['throughput_images_per_min']:.0f}å¼ /åˆ†é’Ÿ\n\n"
        
        # æ€»ç»“
        report += "=" * 80 + "\n"
        report += "æ€»ç»“\n"
        report += "=" * 80 + "\n"
        
        passed = 0
        total = 0
        
        if "npu_inference" in self.results:
            total += 1
            if self.results["npu_inference"]["avg_latency_ms"] < 500:
                passed += 1
        
        if "vector_retrieval" in self.results and "error" not in self.results["vector_retrieval"]:
            total += 1
            if self.results["vector_retrieval"]["avg_latency_ms"] < 100:
                passed += 1
        
        if "batch_processing" in self.results:
            total += 1
            if self.results["batch_processing"]["throughput_files_per_min"] >= 1000:
                passed += 1
        
        if "ocr_performance" in self.results:
            total += 1
            if self.results["ocr_performance"]["avg_time_per_image_ms"] < 500:
                passed += 1
        
        report += f"è¾¾æ ‡é¡¹ç›®: {passed}/{total}\n"
        
        if passed == total:
            report += "ğŸ‰ æ‰€æœ‰æ€§èƒ½æŒ‡æ ‡å‡è¾¾æ ‡ï¼\n"
        else:
            report += f"[WARN] {total - passed} é¡¹æ€§èƒ½æŒ‡æ ‡æœªè¾¾æ ‡\n"
        
        return report


if __name__ == "__main__":
    # åˆ›å»ºæ€§èƒ½æµ‹è¯•å®ä¾‹
    perf_test = PerformanceTest()
    
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    perf_test.test_npu_inference(iterations=10)
    perf_test.test_vector_retrieval(iterations=100)
    perf_test.test_batch_processing(file_count=100)
    perf_test.test_ocr_performance(image_count=50)
    
    # ç”ŸæˆæŠ¥å‘Š
    print("\n" + "=" * 80)
    report = perf_test.generate_report()
    print(report)
