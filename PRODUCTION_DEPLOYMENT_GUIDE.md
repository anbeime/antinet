# ğŸš€ AntiNet AI PC - ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²æŒ‡å—

## âœ… ä¼˜åŒ–å®ŒæˆçŠ¶æ€

**ä¼˜åŒ–ç‰ˆæœ¬**: v3.0 (BURST Mode)  
**å®Œæˆæ—¶é—´**: 2026-01-27  
**æ€§èƒ½æå‡**: 49-56%  
**ç”Ÿäº§å°±ç»ª**: âœ… æ˜¯

---

## ğŸ“Š å½“å‰æ€§èƒ½åŸºå‡†

### **æ€§èƒ½æŒ‡æ ‡**

| Token æ•° | å»¶è¿Ÿ | çŠ¶æ€ | ååé‡ |
|---------|------|------|--------|
| 8       | 533ms | âœ… ä¼˜ç§€ | ~1.9 req/s |
| 16      | 625ms | âœ… è‰¯å¥½ | ~1.6 req/s |
| 24      | 1747ms | âš ï¸ å¯æ¥å— | ~0.6 req/s |

### **æ¨èé…ç½®**
- **é»˜è®¤ Token æ•°**: 16
- **æœ€å¤§ Token æ•°**: 24
- **ç†”æ–­é˜ˆå€¼**: 3000ms
- **æ€§èƒ½æ¨¡å¼**: BURST

---

## ğŸ¯ åº”ç”¨åœºæ™¯é…ç½®

### **åœºæ™¯ 1ï¼šèŠå¤©æœºå™¨äººï¼ˆæ¨èï¼‰**

```python
# backend/routes/chat_routes.py
@router.post("/chat")
async def chat(message: str):
    loader = get_model_loader()
    
    # æ¨èé…ç½®ï¼š16 tokens
    response = loader.infer(
        prompt=message,
        max_new_tokens=16,  # ~625ms
        temperature=0.7
    )
    
    return {"response": response}
```

**æ€§èƒ½**ï¼š
- å»¶è¿Ÿ: ~625ms
- ç”¨æˆ·ä½“éªŒ: ä¼˜ç§€
- é€‚ç”¨: å¿«é€Ÿé—®ç­”ã€ç®€çŸ­å¯¹è¯

---

### **åœºæ™¯ 2ï¼šæ•°æ®åˆ†æå»ºè®®**

```python
# backend/routes/analysis_routes.py
@router.post("/analyze")
async def analyze_data(data: dict):
    loader = get_model_loader()
    
    # åˆ†æåœºæ™¯ï¼š16-24 tokens
    prompt = f"åˆ†ææ•°æ®: {data}"
    response = loader.infer(
        prompt=prompt,
        max_new_tokens=20,  # ~800-1000ms
        temperature=0.5
    )
    
    return {"analysis": response}
```

**æ€§èƒ½**ï¼š
- å»¶è¿Ÿ: ~800-1000ms
- ç”¨æˆ·ä½“éªŒ: è‰¯å¥½
- é€‚ç”¨: æ•°æ®åˆ†æã€å»ºè®®ç”Ÿæˆ

---

### **åœºæ™¯ 3ï¼šçŸ¥è¯†é—®ç­”**

```python
# backend/routes/knowledge_routes.py
@router.post("/qa")
async def question_answer(question: str):
    loader = get_model_loader()
    
    # é—®ç­”åœºæ™¯ï¼š8-16 tokens
    response = loader.infer(
        prompt=question,
        max_new_tokens=12,  # ~550-600ms
        temperature=0.3
    )
    
    return {"answer": response}
```

**æ€§èƒ½**ï¼š
- å»¶è¿Ÿ: ~550-600ms
- ç”¨æˆ·ä½“éªŒ: ä¼˜ç§€
- é€‚ç”¨: å¿«é€Ÿé—®ç­”ã€çŸ¥è¯†æ£€ç´¢

---

## ğŸ”§ API æœ€ä½³å®è·µ

### **1. åŠ¨æ€ Token é…ç½®**

```python
# æ ¹æ®åœºæ™¯åŠ¨æ€è°ƒæ•´
def get_optimal_tokens(scenario: str) -> int:
    """æ ¹æ®åœºæ™¯è¿”å›æœ€ä¼˜ token æ•°"""
    token_map = {
        "quick_chat": 8,      # 533ms
        "normal_chat": 16,    # 625ms
        "analysis": 20,       # ~900ms
        "detailed": 24,       # 1747ms
    }
    return token_map.get(scenario, 16)

# ä½¿ç”¨ç¤ºä¾‹
@router.post("/smart_chat")
async def smart_chat(message: str, scenario: str = "normal_chat"):
    loader = get_model_loader()
    tokens = get_optimal_tokens(scenario)
    
    response = loader.infer(
        prompt=message,
        max_new_tokens=tokens,
        temperature=0.7
    )
    
    return {
        "response": response,
        "tokens_used": tokens,
        "scenario": scenario
    }
```

---

### **2. æ€§èƒ½ç›‘æ§**

```python
import time
import logging

logger = logging.getLogger(__name__)

def monitored_infer(loader, prompt: str, max_tokens: int):
    """å¸¦æ€§èƒ½ç›‘æ§çš„æ¨ç†"""
    start = time.time()
    
    try:
        response = loader.infer(prompt, max_new_tokens=max_tokens)
        latency = (time.time() - start) * 1000
        
        # è®°å½•æ€§èƒ½æŒ‡æ ‡
        logger.info(f"Inference: {latency:.0f}ms, tokens: {max_tokens}")
        
        # æ€§èƒ½å‘Šè­¦
        if latency > 1000:
            logger.warning(f"High latency detected: {latency:.0f}ms")
        
        return response, latency
        
    except Exception as e:
        logger.error(f"Inference failed: {e}")
        raise
```

---

### **3. é”™è¯¯å¤„ç†å’Œé™çº§**

```python
from fastapi import HTTPException

async def safe_infer(prompt: str, max_tokens: int = 16):
    """å®‰å…¨çš„æ¨ç†è°ƒç”¨ï¼Œå¸¦é™çº§ç­–ç•¥"""
    loader = get_model_loader()
    
    try:
        # å°è¯•æ­£å¸¸æ¨ç†
        response = loader.infer(prompt, max_new_tokens=max_tokens)
        return response
        
    except RuntimeError as e:
        if "ç†”æ–­æ£€æŸ¥å¤±è´¥" in str(e):
            # é™çº§ï¼šå‡å°‘ token æ•°é‡è¯•
            logger.warning(f"Circuit breaker triggered, retrying with fewer tokens")
            response = loader.infer(prompt, max_new_tokens=8)
            return response
        else:
            raise
            
    except Exception as e:
        logger.error(f"Inference error: {e}")
        raise HTTPException(status_code=500, detail="æ¨ç†æœåŠ¡æš‚æ—¶ä¸å¯ç”¨")
```

---

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–æŠ€å·§

### **1. æç¤ºè¯ä¼˜åŒ–**

```python
# âŒ ä¸æ¨èï¼šå†—é•¿çš„æç¤ºè¯
prompt = "ä½ å¥½ï¼Œæˆ‘æ˜¯ç”¨æˆ·ï¼Œæˆ‘æƒ³é—®ä½ ä¸€ä¸ªé—®é¢˜ï¼Œè¯·ä½ å¸®æˆ‘è¯¦ç»†åˆ†æä¸€ä¸‹è¿™ä¸ªæ•°æ®"

# âœ… æ¨èï¼šç®€æ´çš„æç¤ºè¯
prompt = "åˆ†ææ•°æ®"
```

**æ•ˆæœ**ï¼š
- å‡å°‘è¾“å…¥å¤„ç†æ—¶é—´
- é™ä½æ¨ç†å¤æ‚åº¦
- æå‡å“åº”é€Ÿåº¦

---

### **2. æ‰¹é‡æ¨ç†ä¼˜åŒ–**

```python
async def batch_infer(prompts: list[str], max_tokens: int = 16):
    """æ‰¹é‡æ¨ç†ï¼ˆä¸²è¡Œï¼‰"""
    loader = get_model_loader()
    results = []
    
    for prompt in prompts:
        response = loader.infer(prompt, max_new_tokens=max_tokens)
        results.append(response)
    
    return results

# ä½¿ç”¨ç¤ºä¾‹
prompts = ["é—®é¢˜1", "é—®é¢˜2", "é—®é¢˜3"]
responses = await batch_infer(prompts, max_tokens=12)
```

**æ³¨æ„**ï¼š
- å½“å‰ä¸ºä¸²è¡Œå¤„ç†
- æ¯ä¸ªè¯·æ±‚ ~600ms
- 3ä¸ªè¯·æ±‚çº¦ 1.8s

---

### **3. ç¼“å­˜ç­–ç•¥**

```python
from functools import lru_cache
import hashlib

# ç®€å•ç¼“å­˜
response_cache = {}

def cached_infer(prompt: str, max_tokens: int = 16):
    """å¸¦ç¼“å­˜çš„æ¨ç†"""
    # ç”Ÿæˆç¼“å­˜é”®
    cache_key = hashlib.md5(
        f"{prompt}_{max_tokens}".encode()
    ).hexdigest()
    
    # æ£€æŸ¥ç¼“å­˜
    if cache_key in response_cache:
        logger.info(f"Cache hit for: {prompt[:20]}...")
        return response_cache[cache_key]
    
    # æ‰§è¡Œæ¨ç†
    loader = get_model_loader()
    response = loader.infer(prompt, max_new_tokens=max_tokens)
    
    # å­˜å…¥ç¼“å­˜
    response_cache[cache_key] = response
    
    return response
```

**æ•ˆæœ**ï¼š
- ç›¸åŒé—®é¢˜å³æ—¶è¿”å›
- å‡å°‘ NPU è´Ÿè½½
- æå‡ç”¨æˆ·ä½“éªŒ

---

## ğŸ” ç›‘æ§å’Œæ—¥å¿—

### **1. æ€§èƒ½æŒ‡æ ‡æ”¶é›†**

```python
# backend/services/metrics.py
from collections import defaultdict
import time

class PerformanceMetrics:
    def __init__(self):
        self.latencies = []
        self.token_counts = defaultdict(list)
        self.errors = 0
    
    def record_inference(self, latency: float, tokens: int):
        """è®°å½•æ¨ç†æ€§èƒ½"""
        self.latencies.append(latency)
        self.token_counts[tokens].append(latency)
    
    def record_error(self):
        """è®°å½•é”™è¯¯"""
        self.errors += 1
    
    def get_stats(self):
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        if not self.latencies:
            return {}
        
        return {
            "total_requests": len(self.latencies),
            "avg_latency": sum(self.latencies) / len(self.latencies),
            "min_latency": min(self.latencies),
            "max_latency": max(self.latencies),
            "errors": self.errors,
            "by_tokens": {
                tokens: {
                    "count": len(lats),
                    "avg": sum(lats) / len(lats)
                }
                for tokens, lats in self.token_counts.items()
            }
        }

# å…¨å±€å®ä¾‹
metrics = PerformanceMetrics()
```

---

### **2. å¥åº·æ£€æŸ¥ç«¯ç‚¹**

```python
# backend/routes/health_routes.py
from fastapi import APIRouter

router = APIRouter()

@router.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    try:
        loader = get_model_loader()
        
        # å¿«é€Ÿæ¨ç†æµ‹è¯•
        start = time.time()
        response = loader.infer("test", max_new_tokens=8)
        latency = (time.time() - start) * 1000
        
        status = "healthy" if latency < 1000 else "degraded"
        
        return {
            "status": status,
            "latency_ms": latency,
            "model_loaded": loader.is_loaded,
            "burst_mode": "enabled"
        }
        
    except Exception as e:
        return {
            "status": "unhealthy",
            "error": str(e)
        }

@router.get("/metrics")
async def get_metrics():
    """è·å–æ€§èƒ½æŒ‡æ ‡"""
    return metrics.get_stats()
```

---

## ğŸš¨ å‘Šè­¦å’Œæ•…éšœå¤„ç†

### **1. æ€§èƒ½å‘Šè­¦**

```python
def check_performance_alert(latency: float, tokens: int):
    """æ£€æŸ¥æ€§èƒ½å‘Šè­¦"""
    # å®šä¹‰é˜ˆå€¼
    thresholds = {
        8: 800,    # 8 tokens åº” < 800ms
        16: 1000,  # 16 tokens åº” < 1000ms
        24: 2500,  # 24 tokens åº” < 2500ms
    }
    
    threshold = thresholds.get(tokens, 3000)
    
    if latency > threshold:
        logger.warning(
            f"Performance alert: {latency:.0f}ms > {threshold}ms "
            f"for {tokens} tokens"
        )
        # å¯ä»¥å‘é€å‘Šè­¦é€šçŸ¥
        # send_alert(f"High latency: {latency:.0f}ms")
```

---

### **2. è‡ªåŠ¨é‡å¯ç­–ç•¥**

```python
# backend/services/auto_recovery.py
import os
import sys

class AutoRecovery:
    def __init__(self, max_errors: int = 10):
        self.error_count = 0
        self.max_errors = max_errors
    
    def record_error(self):
        """è®°å½•é”™è¯¯"""
        self.error_count += 1
        
        if self.error_count >= self.max_errors:
            logger.critical(
                f"Too many errors ({self.error_count}), "
                f"consider restarting service"
            )
            # å¯ä»¥è§¦å‘è‡ªåŠ¨é‡å¯
            # self.restart_service()
    
    def reset_errors(self):
        """é‡ç½®é”™è¯¯è®¡æ•°"""
        self.error_count = 0
    
    def restart_service(self):
        """é‡å¯æœåŠ¡"""
        logger.info("Restarting service...")
        os.execv(sys.executable, ['python'] + sys.argv)

# å…¨å±€å®ä¾‹
auto_recovery = AutoRecovery()
```

---

## ğŸ“¦ éƒ¨ç½²é…ç½®

### **1. ç¯å¢ƒå˜é‡é…ç½®**

```bash
# .env
# NPU é…ç½®
QNN_LOG_LEVEL=INFO
QNN_PERFORMANCE_MODE=BURST
QNN_HTP_PERFORMANCE_MODE=burst

# æ¨¡å‹é…ç½®
DEFAULT_MODEL=qwen2-7b-ssd
DEFAULT_MAX_TOKENS=16
CIRCUIT_BREAKER_THRESHOLD=3000

# æœåŠ¡é…ç½®
HOST=0.0.0.0
PORT=8000
WORKERS=1  # NPU å•ä¾‹ï¼Œåªç”¨ 1 ä¸ª worker
```

---

### **2. å¯åŠ¨è„šæœ¬**

```bash
# start_production.bat
@echo off
echo Starting AntiNet AI PC - Production Mode
echo.

cd C:\test\antinet
call venv_arm64\Scripts\activate

echo [INFO] Starting backend service...
python backend\main.py

pause
```

---

### **3. ç³»ç»ŸæœåŠ¡é…ç½®ï¼ˆå¯é€‰ï¼‰**

```powershell
# install_service.ps1
# å°† AntiNet å®‰è£…ä¸º Windows æœåŠ¡

$serviceName = "AntiNetAIPC"
$serviceDisplayName = "AntiNet AI PC Service"
$servicePath = "C:\test\antinet\venv_arm64\Scripts\python.exe"
$serviceArgs = "C:\test\antinet\backend\main.py"

# åˆ›å»ºæœåŠ¡
New-Service -Name $serviceName `
    -DisplayName $serviceDisplayName `
    -BinaryPathName "$servicePath $serviceArgs" `
    -StartupType Automatic `
    -Description "AntiNet AI PC NPU Service with BURST mode"

Write-Host "Service installed successfully!"
Write-Host "Start service: Start-Service $serviceName"
```

---

## ğŸ“Š æ€§èƒ½æµ‹è¯•è„šæœ¬

### **å‹åŠ›æµ‹è¯•**

```python
# performance_test.py
import time
import statistics
from concurrent.futures import ThreadPoolExecutor
import sys
sys.path.insert(0, 'C:/test/antinet/backend')

from models.model_loader import get_model_loader

def single_request(prompt: str, tokens: int):
    """å•ä¸ªè¯·æ±‚"""
    loader = get_model_loader()
    start = time.time()
    response = loader.infer(prompt, max_new_tokens=tokens)
    latency = (time.time() - start) * 1000
    return latency

def stress_test(num_requests: int = 10, tokens: int = 16):
    """å‹åŠ›æµ‹è¯•"""
    print(f"Running stress test: {num_requests} requests, {tokens} tokens")
    print("-" * 60)
    
    latencies = []
    
    for i in range(num_requests):
        latency = single_request(f"Test {i}", tokens)
        latencies.append(latency)
        print(f"Request {i+1}/{num_requests}: {latency:.0f}ms")
    
    # ç»Ÿè®¡
    print("\n" + "=" * 60)
    print("Statistics:")
    print(f"  Total requests: {num_requests}")
    print(f"  Average latency: {statistics.mean(latencies):.0f}ms")
    print(f"  Median latency: {statistics.median(latencies):.0f}ms")
    print(f"  Min latency: {min(latencies):.0f}ms")
    print(f"  Max latency: {max(latencies):.0f}ms")
    print(f"  Std deviation: {statistics.stdev(latencies):.0f}ms")
    print("=" * 60)

if __name__ == "__main__":
    stress_test(num_requests=20, tokens=16)
```

---

## ğŸ¯ ç”Ÿäº§ç¯å¢ƒæ£€æŸ¥æ¸…å•

### **éƒ¨ç½²å‰æ£€æŸ¥**

- [ ] BURST æ¨¡å¼å·²å¯ç”¨
- [ ] ç†”æ–­é˜ˆå€¼è®¾ç½®ä¸º 3000ms
- [ ] é»˜è®¤ token æ•°è®¾ç½®ä¸º 16
- [ ] å¥åº·æ£€æŸ¥ç«¯ç‚¹æ­£å¸¸
- [ ] æ—¥å¿—ç³»ç»Ÿé…ç½®å®Œæˆ
- [ ] é”™è¯¯å¤„ç†æœºåˆ¶å°±ç»ª
- [ ] æ€§èƒ½ç›‘æ§å¯ç”¨
- [ ] å¤‡ä»½å’Œæ¢å¤ç­–ç•¥åˆ¶å®š

### **æ€§èƒ½éªŒè¯**

- [ ] 8 tokens < 800ms
- [ ] 16 tokens < 1000ms
- [ ] 24 tokens < 2500ms
- [ ] ç†”æ–­æ£€æŸ¥é€šè¿‡
- [ ] å‹åŠ›æµ‹è¯•é€šè¿‡

### **ç›‘æ§é…ç½®**

- [ ] æ€§èƒ½æŒ‡æ ‡æ”¶é›†
- [ ] é”™è¯¯æ—¥å¿—è®°å½•
- [ ] å‘Šè­¦æœºåˆ¶è®¾ç½®
- [ ] å¥åº·æ£€æŸ¥å®šæœŸæ‰§è¡Œ

---

## ğŸ“ æŠ€æœ¯æ”¯æŒ

### **å¸¸è§é—®é¢˜**

**Q: æ€§èƒ½çªç„¶ä¸‹é™æ€ä¹ˆåŠï¼Ÿ**
A: 
1. æ£€æŸ¥ NPU é©±åŠ¨çŠ¶æ€
2. é‡å¯åç«¯æœåŠ¡
3. æŸ¥çœ‹é”™è¯¯æ—¥å¿—
4. è¿è¡Œå¥åº·æ£€æŸ¥

**Q: å¦‚ä½•è¿›ä¸€æ­¥ä¼˜åŒ–ï¼Ÿ**
A:
1. åˆ‡æ¢åˆ° Llama3.2-3B æ¨¡å‹
2. å‡å°‘ max_tokens
3. å¯ç”¨ç¼“å­˜ç­–ç•¥
4. ä¼˜åŒ–æç¤ºè¯

**Q: å¦‚ä½•ç›‘æ§ç”Ÿäº§ç¯å¢ƒï¼Ÿ**
A:
1. ä½¿ç”¨ /metrics ç«¯ç‚¹
2. å®šæœŸè¿è¡Œå¥åº·æ£€æŸ¥
3. ç›‘æ§é”™è¯¯æ—¥å¿—
4. è®¾ç½®æ€§èƒ½å‘Šè­¦

---

## ğŸ‰ æ€»ç»“

**å½“å‰çŠ¶æ€**: âœ… ç”Ÿäº§å°±ç»ª

**æ€§èƒ½æŒ‡æ ‡**:
- 8-16 tokens: ~600ms (ä¼˜ç§€)
- ååé‡: ~1.6 req/s
- å¯ç”¨æ€§: é«˜

**æ¨èé…ç½®**:
- é»˜è®¤ 16 tokens
- BURST æ€§èƒ½æ¨¡å¼
- 3000ms ç†”æ–­é˜ˆå€¼

**ä¸‹ä¸€æ­¥**:
1. éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ
2. å¯ç”¨ç›‘æ§å’Œæ—¥å¿—
3. å®šæœŸæ€§èƒ½æµ‹è¯•
4. æ ¹æ®å®é™…æƒ…å†µè°ƒä¼˜

---

**AntiNet AI PC å·²å‡†å¤‡å¥½æŠ•å…¥ç”Ÿäº§ä½¿ç”¨ï¼** ğŸš€
