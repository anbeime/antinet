# Antinet 智能知识管家 - AI 开发团队

## 项目使命

基于骁龙 X Elite AIPC 平台，打造端侧智能数据中枢与协同分析平台。通过 NPU 加速的轻量化大模型，实现自然语言驱动的数据分析与四色卡片知识管理，强调数据不出域的隐私保护。

## 高通AIPC赛道核心要求（不可妥协）

### 强制要求
- 必须使用 NPU：模型推理必须在骁龙 Hexagon NPU 上运行
- QAI AppBuilder：使用高通官方工具进行模型部署
- 推理延迟 < 500ms：端侧实时响应，远快于云端
- 数据不出域：所有数据处理在本地完成，保障隐私
- 端侧执行：演示必须在 AIPC 上真实运行，非仅模拟

### 评分关键维度（10%）
1. 模型运行算力单元说明：CPU/GPU/NPU 分工明确
2. 算力选择理由：为什么用 NPU？性能/功耗/体验影响？
3. 端侧运行效果：真实性能数据对比（NPU vs CPU）
4. 异构计算使用：如使用，说明模块划分与调度逻辑

## 核心开发工作流程

Antinet 采用由主编排器管理的结构化开发周期，强调 NPU 性能验证与端侧隐私保护：

### 目标设定
您向主编排器提供高级目标。编排器必须确认：
- 是否涉及 NPU 推理？
- 是否符合数据不出域原则？
- 预期性能指标（延迟/吞吐量）？

### 工作准备（远程 AIPC 专用）⭐️
登录远程 AIPC 后，主编排器必须第一时间执行：

#### 1. 同步代码（必须执行）
```bash
# 首次使用：克隆仓库
git clone https://github.com/anbeime/antinet.git
cd antinet

# 后续使用：拉取最新代码
git pull origin main

# 验证分支状态
git status
git log -1  # 查看最新提交
```

#### 2. 环境验证
```bash
# 验证 Python 版本（必须是 3.12）
python --version

# 验证 QAI AppBuilder 可用性
python -c "import qai_appbuilder; print('QAI AppBuilder 可用')"

# 运行 NPU 性能基准测试
python backend/verify_npu_performance.py
```

#### 3. 任务确认
- 检查上次工作遗留的 TODO 或未完成任务
- 确认本次工作目标和预期成果
- 记录工作开始时间（用于提交信息）

### 规划与设计
Orchestrator 规划阶段，委派任务给专门的骁龙AIPC开发军团：

#### 解决方案架构师（蓝图创作者）
职责：创建技术规范，确保符合高通AIPC技术要求

关键任务：
- 设计系统架构（前端 React + 后端 FastAPI + NPU推理）
- 明确算力分配：CPU（控制逻辑）、GPU（图像处理）、NPU（核心推理）
- 定义模型部署方案：Qwen2-1.5B INT8 量化 -> QNN 格式
- 规划异构计算调度逻辑（如需要）
- 输出架构规范到 .specs/ 目录

输出产物：
.specs/
├── architecture.md          # 系统架构图
├── npu-integration.md       # NPU集成方案
├── model-deployment.md      # 模型部署规范
├── privacy-compliance.md   # 端侧隐私合规检查
└── api-spec.md             # API 接口规范

#### UX 专家（用户倡导者）
职责：设计用户体验，强调端侧响应速度与四色卡片系统

关键任务：
- 设计自然语言查询流程（输入 -> NPU推理 -> 四色卡片输出）
- 设计四色卡片交互（事实/解释/风险/行动）
- 规划 NPU 性能监控界面（实时延迟/吞吐量图表）
- 设计响应式深色模式 UI
- 输出设计规范到 .design/ 目录

输出产物：
.design/
├── user-flows.md            # 用户流程图
├── four-color-cards.md     # 四色卡片设计规范
├── npu-dashboard.md       # NPU性能监控界面设计
├── component-library.md   # Tailwind CSS 组件库
└── animations.md          # Framer Motion 动画规范

### 实施：Apex 实施者（精确构建器）
职责：严格按照规格编写高质量代码，确保符合项目代码风格

#### 前端开发规则
```typescript
import React, { useState } from 'react';
import { motion } from 'framer-motion';
import { BarChart } from 'recharts';

interface Props {
  title: string;
  data: DataType[];
}

export default function AnalyticsReport({ title, data }: Props) {
  const [isLoading, setIsLoading] = useState(false);

  return (
    <motion.div
      initial={{ opacity: 0 }}
      animate={{ opacity: 1 }}
      className="bg-white dark:bg-gray-800 rounded-lg p-6"
    >
      <h2 className="text-2xl font-bold text-gray-900 dark:text-white">
        {title}
      </h2>
    </motion.div>
  );
}
```

#### 后端开发规则
```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, Field

class QueryRequest(BaseModel):
    query: str = Field(..., description="自然语言查询")

@app.post("/api/analyze")
async def analyze_data(request: QueryRequest):
    model = load_model_if_needed()

    start_time = time.time()
    result = model.infer(input_ids=input_ids)
    inference_time = (time.time() - start_time) * 1000

    if inference_time > 500:
        logger.warning(f"推理延迟超标: {inference_time:.2f}ms")

    return AnalysisResult(...)
```

#### 四色卡片系统实现
```typescript
type CardColor = 'blue' | 'green' | 'yellow' | 'red';
type CardCategory = '事实' | '解释' | '风险' | '行动';

interface FourColorCard {
  color: CardColor;
  title: string;
  content: string;
  category: CardCategory;
}

const colorMap = {
  blue: 'bg-blue-500',
  green: 'bg-green-500',
  yellow: 'bg-yellow-500',
  red: 'bg-red-500',
};
```

### 验证：守护验证器（独立验证者）

职责：验证实现是否符合规范，重点关注 NPU 性能与隐私合规

#### 验证清单

##### NPU 性能验证
- 推理延迟 < 500ms（实测平均 ~450ms）
- CPU vs NPU 性能对比图表展示加速效果
- 实时性能监控（延迟/吞吐量/QPS）
- 不同输入长度（32/64/128/256 tokens）的基准测试
- 基准测试结果保存到 .benchmarks/

##### 端侧隐私验证
- 所有数据处理在本地完成（无网络请求到云端）
- 数据上传接口仅保存到本地 backend/data/uploads/
- 配置 DATA_STAYS_LOCAL = True 在 backend/config.py
- API 文档明确标注"数据不出域"

##### 功能完整性验证
- 自然语言查询 -> 四色卡片生成
- 四色卡片系统（事实/解释/风险/行动）
- NPU 性能监控仪表板
- 数据分析与可视化（Recharts 图表）
- 团队协作功能
- GTD 任务系统

##### 代码质量验证
- TypeScript strict mode，无 any 类型
- Pydantic 数据验证完整
- API 路由使用 /api 前缀
- Tailwind CSS 样式（避免内联样式）
- 错误处理完善（HTTP 适当状态码）

### 迭代：根据验证结果优化
- 修复功能缺陷 -> Apex 实施者
- 优化 NPU 性能（如延迟超标）-> NPU 优化专家
- 隐私合规检查 -> 端侧隐私守护者
- 性能基准测试 -> 守护验证器

### 工作收尾（远程 AIPC 专用）⭐️
时间段结束前 15 分钟，主编排器必须执行：

#### 1. 代码提交（强制要求）
```bash
# 检查修改内容
git status
git diff

# 暂存所有修改
git add .

# 提交更改（包含详细信息）
git commit -m "$(cat <<'EOF'
feat: [功能描述]

工作时段: 2026-01-10 04:00-12:00
完成内容:
- 实现 XXX 功能
- 优化 NPU 推理性能（延迟从 XXX ms 降至 XXX ms）
- 修复 Bug XXX

性能数据:
- NPU 推理延迟: XXX ms
- CPU vs NPU 加速比: X.Xx

Co-Authored-By: Claude Sonnet 4.5 <noreply@anthropic.com>
EOF
)"

# 推送到远程（必须完成）
git push origin main

# 验证推送成功
git log -1
```

#### 2. 工作记录
- 记录本次完成的功能和遗留问题
- 更新 TODO 列表（标记完成/待办）
- 记录性能数据变化

#### 3. 环境清理
- 清理临时文件和缓存
- 确保不遗留敏感信息
- 退出登录

#### 4. 应急预案
```bash
# 如果时间紧急，快速保存
git add . && git commit -m "WIP: 时间到，临时保存" && git push

# 如果推送失败（冲突）
git pull --rebase origin main
# 解决冲突后
git add <resolved-files>
git rebase --continue
git push origin main
```

## 认识团队（骁龙AIPC开发军团）

### 主编排器（指挥者）
职责：项目负责人和核心协调者
- 管理最小化编排，委派任务
- 确认每个任务符合高通AIPC要求
- 与您沟通进度和技术决策
- 处理简单问题（如路由配置、样式调整）

关键判断：
任务是否涉及：
- NPU 推理 -> 委派给 NPU 优化专家
- 数据处理 -> 确认数据不出域原则
- 性能优化 -> 要求提供性能基准数据
- 新增 API -> 要求符合 FastAPI 规范

### 解决方案架构师（蓝图创作者）
职责：技术设计专家，确保架构符合高通平台要求

设计原则：
1. 算力分离明确：
   - CPU：控制逻辑、数据预处理（~20%）
   - GPU：图像处理、并行计算（如需要）
   - NPU：核心模型推理（~60-70%）

2. 端侧优先：
   - 优先本地处理，避免云端依赖
   - 数据上传仅保存到本地目录
   - 模型文件本地加载（不提交到 Git）

3. 性能优先：
   - 模型 INT8 量化，减小体积提升速度
   - 推理延迟目标 < 500ms
   - CPU/GPU/NPU 异构协同

输出产物：
.specs/
├── architecture.md
│   ├── 系统架构图（前端->后端->NPU）
│   ├── 算力分配表（CPU/GPU/NPU）
│   └── 技术栈版本（React 18, FastAPI 0.109, QAI AppBuilder 2.31.0）
├── npu-integration.md
│   ├── QNN 模型转换流程
│   ├── QAI AppBuilder API 使用示例
│   ├── 性能优化策略（量化、算子融合）
│   └── 故障排查指南
└── api-spec.md
    ├── /api/analyze - 自然语言分析
    ├── /api/performance/benchmark - 性能基准测试
    └── /api/data/upload - 数据上传（本地）

### UX 专家（用户倡导者）
职责：用户体验设计，强调端侧响应速度与四色卡片系统

设计风格：
- 简约现代：Tailwind CSS 原子化样式
- 深色模式支持：使用 dark: 前缀
- 流畅动画：Framer Motion（淡入/缩放/悬停效果）
- 数据可视化：Recharts（折线图/柱状图/饼图）

四色卡片设计：
```tsx
// 蓝色卡片 - 事实
<div className="bg-blue-500 text-white p-4 rounded-lg">
  <h3 className="font-bold">数据事实</h3>
  <p>{factContent}</p>
</div>

// 绿色卡片 - 解释
<div className="bg-green-500 text-white p-4 rounded-lg">
  <h3 className="font-bold">原因解释</h3>
  <p>{explanationContent}</p>
</div>

// 黄色卡片 - 风险
<div className="bg-yellow-500 text-white p-4 rounded-lg">
  <h3 className="font-bold">风险预警</h3>
  <p>{riskContent}</p>
</div>

// 红色卡片 - 行动
<div className="bg-red-500 text-white p-4 rounded-lg">
  <h3 className="font-bold">行动建议</h3>
  <p>{actionContent}</p>
</div>
```

NPU 性能监控界面：
- 实时延迟趋势图（LineChart）
- CPU vs NPU 性能对比（BarChart）
- 吞吐量（QPS）实时显示
- 系统健康状态（内存/算力占用）

### Apex 实施者（精确构建器）
职责：严格按照规格编写高质量代码

前端代码规范：
```typescript
import React, { useState, useEffect } from 'react';
import { motion, AnimatePresence } from 'framer-motion';
import { BarChart, LineChart, XAxis, YAxis, Tooltip, ResponsiveContainer } from 'recharts';

interface Props {
  title: string;
  data: DataType[];
  onAction?: () => void;
}

export default function Component({ title, data, onAction }: Props) {
  const [isLoading, setIsLoading] = useState(false);

  useEffect(() => {
    // 数据加载
  }, []);

  return (
    <motion.div
      initial={{ opacity: 0, y: 20 }}
      animate={{ opacity: 1, y: 0 }}
      className="bg-white dark:bg-gray-800 rounded-lg shadow-md p-6"
    >
    </motion.div>
  );
}
```

后端代码规范：
```python
from fastapi import FastAPI, HTTPException, UploadFile, File
from pydantic import BaseModel, Field
import logging
import time

app = FastAPI(title="Antinet API", version="1.0.0")
logger = logging.getLogger(__name__)

class QueryRequest(BaseModel):
    query: str = Field(..., description="自然语言查询")
    context: dict = Field(default_factory=dict, description="上下文信息")

@app.post("/api/analyze")
async def analyze_data(request: QueryRequest):
    """数据分析接口 - 核心功能"""
    try:
        model = load_model_if_needed()

        start_time = time.time()
        result = model.infer(input_ids=input_ids)
        inference_time = (time.time() - start_time) * 1000

        logger.info(f"NPU推理延迟: {inference_time:.2f}ms")

        cards = generate_four_color_cards(result)

        return AnalysisResult(cards=cards, performance=inference_time)

    except Exception as e:
        logger.error(f"分析失败: {e}")
        raise HTTPException(status_code=500, detail=str(e))
```

### 守护验证器（独立验证者）
职责：验证实现是否符合规范

验证脚本示例：
```python
def verify_npu_performance():
    """验证 NPU 推理延迟 < 500ms"""
    model = load_model_if_needed()

    latencies = []
    for _ in range(10):
        start = time.time()
        model.infer(input_ids=input_ids)
        latency = (time.time() - start) * 1000
        latencies.append(latency)

    avg_latency = sum(latencies) / len(latencies)

    assert avg_latency < 500, f"NPU推理延迟超标: {avg_latency:.2f}ms"
    logger.info(f"✓ NPU性能验证通过: {avg_latency:.2f}ms")

def verify_data_stays_local():
    """验证所有数据处理在本地完成"""
    from backend.config import settings

    assert settings.DATA_STAYS_LOCAL is True
    logger.info("✓ 数据不出域验证通过")
```

### 端侧隐私守护者（专项验证者）
职责：确保符合数据不出域原则

检查清单：
- API 路由不包含云端回调
- 数据上传仅保存到 backend/data/uploads/
- 配置 DATA_STAYS_LOCAL = True
- 模型推理在本地 NPU 执行
- 无第三方数据分析服务

### NPU 优化专家（性能调优专家）
职责：优化 NPU 推理性能

优化策略：
1. 模型量化：
   - FP32 -> INT8 量化（体积减小 75%）
   - 混合量化（核心层 FP16，普通层 INT8）

2. 输入优化：
   - 减小序列长度（Context Length 优化）
   - 批处理（Batch Size = 1，端侧推荐）

3. 算子融合：
   - QNN SDK 自动优化
   - 减少内存拷贝

4. 性能模式：
   - Burst 模式（高性能，高功耗）
   - Power Saver 模式（低功耗）
   - Default 模式（平衡）

### DocuCrafter（Markdown 文档生成器）
职责：生成项目文档，主要在 .docs/ 目录

文档任务：
- init：初始化文档结构
- update：更新文档与代码同步
- api-docs：生成 API 文档
- deploy-guide：部署到 AIPC 指南

输出产物：
.docs/
├── README.md                   # 项目概述
├── SETUP.md                   # 环境配置
├── DEPLOY_TO_AIPC.md          # AIPC 部署指南
├── NPU_PERFORMANCE.md         # NPU 性能说明
├── PRIVACY_COMPLIANCE.md      # 端侧隐私合规
├── API_REFERENCE.md           # API 文档
└── TROUBLESHOOTING.md          # 故障排查

## 远程 AIPC 开发规范

### 设备使用规则

#### 基本情况
- **复赛周期**: 2026 年 1 月 10 日 - 2 月 5 日（共 27 天）
- **设备数量**: 20 台 AIPC，60 支团队
- **使用方式**: 每 3 支团队共用 1 台 AIPC
- **使用规则**: 9×24 小时轮转，每队每日 8 小时

#### 分组与轮换时间表

| 时间段 | 1月10日-1月18日 | 1月19日-1月27日 | 1月28日-2月5日 |
|--------|----------------|----------------|---------------|
| 第一组 | 04:00-12:00    | 12:00-20:00    | 20:00-04:00   |
| 第二组 | 12:00-20:00    | 20:00-04:00    | 04:00-12:00   |
| 第三组 | 20:00-04:00    | 04:00-12:00    | 12:00-20:00   |

⚠️ **请务必按照分配的组别与时间段使用，错峰使用将影响其他队伍并可能被限制权限。**

### Git 工作流（强制执行）⭐️

远程 AIPC 为共享设备，数据随时可能丢失，**Git 推送是唯一安全的数据保护方式**。

#### 每次登录后（第一件事）
```bash
# 1. 克隆仓库（首次使用）
git clone https://github.com/anbeime/antinet.git
cd antinet

# 2. 拉取最新代码（后续使用）
git pull origin main

# 3. 验证分支状态
git status
git log -3  # 查看最近 3 次提交
```

#### 工作期间（每 2 小时推送一次）
```bash
# 阶段性提交（功能完成后立即执行）
git add .
git commit -m "feat: 完成 XXX 功能"
git push origin main
```

#### 工作结束前（时间段结束前 15 分钟）⭐️
```bash
# 1. 检查修改
git status
git diff

# 2. 提交所有更改
git add .

# 3. 编写详细的提交信息
git commit -m "$(cat <<'EOF'
feat: [功能描述]

工作时段: 2026-01-XX XX:XX-XX:XX
完成内容:
- 实现 XXX 功能
- 优化 NPU 推理性能（延迟: XXX ms -> XXX ms）
- 修复 Bug XXX
- 更新文档 XXX

性能数据:
- NPU 推理延迟: XXX ms
- CPU vs NPU 加速比: X.Xx
- 内存占用: XXX MB

遗留问题:
- [ ] 待解决的问题 1
- [ ] 待解决的问题 2

Co-Authored-By: Claude Sonnet 4.5 <noreply@anthropic.com>
EOF
)"

# 4. 推送到远程（必须完成）
git push origin main

# 5. 验证推送成功
git log -1
echo "✓ 代码已安全推送到远程仓库"
```

#### Git 提交类型规范
```
feat:     新功能
fix:      Bug 修复
perf:     性能优化
refactor: 代码重构
docs:     文档更新
test:     测试相关
chore:    构建/工具配置
style:    代码格式调整
WIP:      工作进行中（临时保存）
```

#### 异常处理

**场景 1: 时间到了代码未推送**
```bash
# 10 秒内快速保存（不要手动编辑提交信息）
git add . && git commit -m "WIP: 时间到，临时保存 $(date +%Y-%m-%d_%H:%M)" && git push
```

**场景 2: 推送失败（冲突）**
```bash
# 1. 拉取远程更改
git pull --rebase origin main

# 2. 如果有冲突
git status  # 查看冲突文件

# 3. 快速解决冲突（选择保留版本）
# 保留自己的版本
git checkout --ours <conflicted-file>
# 或保留远程版本
git checkout --theirs <conflicted-file>

# 4. 继续 rebase
git add <resolved-files>
git rebase --continue

# 5. 推送
git push origin main
```

**场景 3: 忘记推送，代码丢失**
- ❌ 远程设备不保证数据安全，丢失代码**无法找回**
- ✅ 预防措施：每 2 小时推送一次，工作结束前必须推送

### 远程桌面重定向（推荐）

通过远程桌面将本地电脑磁盘映射到 AIPC，实现双重备份：

**Windows 本地电脑**
```bash
# 1. 打开远程桌面（Win+R -> mstsc.exe）
# 2. 显示选项 -> 本地资源 -> 详细信息
# 3. 选择需要重定向的本地磁盘
# 4. 连接后，AIPC 可访问本地磁盘（如 \\tsclient\C）

# 额外备份到本地（工作结束时）
xcopy Z:\antinet C:\Backup\antinet-%date:~0,4%%date:~5,2%%date:~8,2% /E /H /Y
```

**Mac 本地电脑**
```bash
# 1. 下载 Windows App（App Store）
# 2. 添加电脑 -> 文件夹 -> 重定向文件夹 -> 选择本地目录
# 3. 连接后，AIPC 可访问重定向文件夹

# 额外备份到本地（工作结束时）
rsync -av ~/Desktop/antinet ~/Backup/antinet-$(date +%Y%m%d)/
```

### 禁止行为（重要）⚠️

#### Git 相关禁止
- ❌ **不推送直接退出登录** - 代码将丢失，无法找回
- ❌ **强制推送覆盖他人代码** (`git push --force`) - 破坏团队协作
- ❌ **依赖远程设备本地存储** - 随时可能被清理
- ❌ **提交敏感信息到 Git** - 密钥、账号密码等

#### 设备使用禁止
- ❌ **在非分配时间段登录** - 影响其他队伍
- ❌ **关机、休眠或影响他人使用**
- ❌ **修改系统配置或更新 Windows**
- ❌ **安装垃圾软件、病毒、木马**
- ❌ **修改/删除共享文件夹数据** (C:\ai-engine-direct-helper)

#### 违规处理
1. 警告
2. 取消远程访问资格
3. 追究责任的权利

## 快速开始

### AIPC 环境配置（远程设备专用）

#### 1. 验证 Python 环境
```bash
# 打开命令提示符（Win+R -> cmd）
python --version
# 必须输出: Python 3.12.x

# 如果版本不对，在 Microsoft Store 安装 Python 3.12
# Win+R -> ms-windows-store://search/?query=python3.12
```

#### 2. 拷贝示例文件到本地桌面
```bash
# 拷贝 ai-engine-direct-helper 到本地桌面（不要剪切，不要修改）
xcopy C:\ai-engine-direct-helper %USERPROFILE%\Desktop\ai-engine-direct-helper\ /E /H

# 切换到 samples 目录
cd %USERPROFILE%\Desktop\ai-engine-direct-helper\samples
```

#### 3. 安装 QAI AppBuilder
```bash
# 安装 QAI AppBuilder 2.31.0（Python 3.12 版本）
pip install qai_appbuilder-2.31.0-cp312-cp312-win_amd64.whl
```

#### 4. 安装依赖包（使用阿里云镜像加速）
```bash
pip install uvicorn py3-wget pydantic_settings fastapi langchain langchain_core langchain_community sse_starlette pypdf python-pptx docx2txt openai requests wget tqdm importlib-metadata qai-hub qai_hub_models huggingface_hub Pillow numpy opencv-python torch torchvision torchaudio transformers diffusers basicsr ultralytics==8.0.193 gradio==5.30.0 -i https://mirrors.aliyun.com/pypi/simple/
```

#### 5. 验证环境
```bash
# 测试示例（注意左图无眼镜，右图有眼镜）
python python\aotgan\aotgan.py

# 如果成功，说明 NPU 环境配置正确
```

### 项目环境准备
```bash
# 克隆/拉取项目代码
git clone https://github.com/anbeime/antinet.git
cd antinet
git pull origin main

# 前端
pnpm install
pnpm dev

# 后端
cd backend
pip install -r requirements.txt
pip install %USERPROFILE%\Desktop\ai-engine-direct-helper\samples\qai_appbuilder-2.31.0-cp312-cp312-win_amd64.whl
python main.py
```

### 验证 NPU 功能
```bash
# 运行性能基准测试
curl http://localhost:8000/api/performance/benchmark

# 验证推理延迟 < 500ms
python backend/verify_npu_performance.py
```

## 复赛提交材料检查清单

### 必交材料（缺一不可）

#### 1. 演示视频（≤3 分钟）⭐️
- ✅ 展示真实运行效果（非仅 PPT 截图）
- ✅ 覆盖核心功能与亮点
- ✅ 说明运行平台（骁龙 X Elite AIPC）
- ✅ 展示 NPU 性能监控数据
- ✅ 视频时长 ≤ 3 分钟（超出仅评审前 3 分钟）

**推荐内容结构**:
```
0:00 - 0:30  项目介绍（背景、痛点、解决方案）
0:30 - 1:30  核心功能演示（自然语言查询 -> 四色卡片）
1:30 - 2:15  NPU 性能展示（性能对比图表、实时监控）
2:15 - 3:00  技术亮点总结（算力分配、端侧隐私保护）
```

#### 2. PPT 概述⭐️
必须包含以下内容：

**项目背景与应用场景**
- 目标用户和核心需求
- 现有解决方案的不足
- 本项目的创新点

**技术架构与算法说明**
- 系统架构图（前端 -> 后端 -> NPU）
- 技术栈版本说明
- 四色卡片系统设计

**AIPC 端侧部署与性能表现（10% 评分权重）**⭐️
- ✅ **模型运行算力单元说明**: CPU（控制逻辑）、GPU（图像处理）、NPU（核心推理）
- ✅ **算力选择理由**: 为什么用 NPU？性能/功耗/体验影响分析
- ✅ **端侧运行效果**: 真实性能数据对比（NPU vs CPU）
  - 推理延迟对比（目标 < 500ms）
  - 加速比数据（目标 > 2x）
  - 内存占用对比
- ✅ **异构计算使用情况**（如有）: 说明模块划分与调度逻辑

### 选交材料（强烈推荐）

#### 3. 应用安装软件包
- 完整的安装包（Windows .exe 或 .msi）
- 安装方法说明文档（一步步截图）

#### 4. 项目技术文档与代码
- 技术文档（架构设计、API 文档、部署指南）
- 完整代码资源包（包含 README 和依赖说明）

### 提交格式要求

```
文件命名: AIPC-通用赛-[团队名称].zip
文件大小: ≤ 200MB（超出附云盘链接）

推荐 ZIP 内部结构:
/
├── Video/
│   └── 演示视频.mp4
├── PPT/
│   └── 项目说明.pptx
├── Doc/（可选）
│   ├── 技术文档.pdf
│   ├── 安装说明.md
│   └── API_REFERENCE.md
└── Code/（可选）
    ├── frontend/
    ├── backend/
    └── README.md
```

### 提交方式与时间
- **提交地址**: https://qc-ai-challenge.cvmart.net/2025
- **提交路径**: 进入官网 -> 选择入围赛题 -> 作品提交 -> 提交作品
- **截止时间**: 2026 年 2 月 5 日 21:00（务必提前 24 小时完成）
- **提交次数**: 仅可提交 1 次（请务必检查完整后再提交）

⚠️ **常见错误避免**:
- ❌ 视频仅为 PPT 翻页，无真实运行画面
- ❌ 未说明模型运行在 CPU/GPU/NPU
- ❌ 文件命名不规范或拆分为多个压缩包
- ❌ 截止时间临近提交失败，无法补交

## 复赛评审维度

评审将从以下维度进行综合评分（总分 100%）：

| 评审维度 | 权重 | 关键要点 |
|---------|------|---------|
| **技术实现与工程能力** | 10% | 代码质量、架构合理性、工程化水平 |
| **高通平台及技术适配度** ⭐️ | 10% | NPU 使用、算力分配、性能优化 |
| **创新性与前瞻性** | 20% | 技术创新、应用创新、未来潜力 |
| **商业应用与场景价值** | 20% | 实用性、市场需求、商业价值 |
| **产品设计与用户体验** | 20% | 界面设计、交互体验、易用性 |
| **完整度与稳定性** | 20% | 功能完整、运行稳定、边界处理 |

### 高通平台及技术适配度（10% 详细说明）⭐️

此评分维度最容易被忽视，但至关重要：

**评分要点**:
1. **模型运行算力单元说明**（必答）
   - 明确说明哪些模块在 CPU/GPU/NPU 上运行
   - 提供算力分配表或架构图
   - 示例：NPU 70%（模型推理）、CPU 20%（控制逻辑）、GPU 10%（图像处理）

2. **算力选择理由**（必答）
   - 为什么选择 NPU 而非 CPU/GPU？
   - 性能影响：推理速度提升多少？
   - 功耗影响：功耗降低多少？
   - 体验影响：用户体验改善在哪里？

3. **端侧运行效果**（必答）
   - 提供真实性能数据对比（NPU vs CPU）
   - 推理延迟、吞吐量、内存占用
   - 性能监控仪表板截图或视频

4. **异构计算使用情况**（如适用）
   - 如使用 CPU+GPU+NPU 协同，说明调度逻辑
   - 哪些任务在哪个单元执行？为什么？

**得分技巧**:
- ✅ 在 PPT 中单独设置"AIPC 技术适配"章节
- ✅ 提供详细的性能对比图表（柱状图/折线图）
- ✅ 展示 NPU 性能监控仪表板（实时数据）
- ✅ 说明技术选型的思考过程（不仅是结果）

## 关键性能指标

| 指标 | 目标 | 实测 | 验证命令 |
|------|------|------|----------|
| NPU 推理延迟 | < 500ms | ~450ms | /api/performance/benchmark |
| CPU vs NPU 加速比 | > 2x | 3.5x - 5.3x | NPU 性能监控仪表板 |
| 内存占用 | < 2GB | ~1.5GB | 系统资源监控 |
| 端到端分析时间 | < 5分钟 | ~3分钟 | 手动测试 |

## 开发优先级

### 高优先级（必须完成）
- NPU 推理延迟 < 500ms
- 四色卡片系统完整实现
- 数据不出域原则严格执行
- NPU 性能监控仪表板
- 演示视频（<= 3分钟）

### 中优先级（增强功能）
- 异构计算优化（CPU/GPU/NPU 协同）
- 更丰富的数据可视化
- UI 动画优化
- 知识搜索功能

### 低优先级（未来优化）
- 端云协同模式（可选）
- 移动端适配
- 模型微调（如需要）

## 常见陷阱与注意事项

### 开发禁止行为
- ❌ 将数据处理上传到云端 - 违反数据不出域原则
- ❌ 使用 any 类型绕过 TypeScript 检查 - 降低代码质量
- ❌ 关闭 NPU 推理，改用 CPU - 不符合赛道要求
- ❌ 在代码中硬编码密钥 - 安全风险
- ❌ 提交模型文件到 Git（.bin, .onnx）- 仓库过大

### 远程设备禁止（严格执行）
- ❌ **不推送代码直接退出登录** - 代码将丢失，无法找回
- ❌ **强制推送覆盖他人代码** (`git push --force`) - 破坏团队协作
- ❌ **在非分配时间段登录** - 影响其他队伍，可能被限制权限
- ❌ **修改 AIPC 系统配置或更新 Windows** - 影响他人使用
- ❌ **修改/删除共享文件夹数据** (C:\ai-engine-direct-helper) - 破坏公共资源

### 技术注意事项
- ⚠️ QAI AppBuilder 仅在 AIPC 上可用（本地电脑无法运行）
- ⚠️ Python 必须是 3.12 版本（其他版本不兼容）
- ⚠️ 推理延迟可能因输入长度而波动（需测试不同 token 长度）
- ⚠️ 异构计算会增加代码复杂度（非必须不建议使用）
- ⚠️ 远程设备数据随时可能丢失（每 2 小时推送一次代码）

### AIPC 开发常见问题 FAQ

#### 环境问题
**Q: Python 版本不对怎么办？**
A: 必须使用 Python 3.12，可在 Microsoft Store 安装
```bash
# Win+R 运行
ms-windows-store://search/?query=python3.12
```

**Q: QAI AppBuilder 安装失败？**
A: 确认已拷贝 ai-engine-direct-helper 到桌面，且使用正确的 .whl 文件路径
```bash
# 正确路径
pip install %USERPROFILE%\Desktop\ai-engine-direct-helper\samples\qai_appbuilder-2.31.0-cp312-cp312-win_amd64.whl
```

**Q: 提示找不到 qai_appbuilder 模块？**
A: 检查是否在正确的 Python 环境中安装，运行 `pip list | findstr qai` 验证

#### 性能问题
**Q: 推理延迟超过 500ms？**
A: 检查是否使用 NPU（非 CPU），检查模型量化是否正确（INT8/INT4）
```python
# 验证 NPU 使用
model = load_model_if_needed()
print(f"运行设备: {model.device}")  # 应输出 NPU
```

**Q: CPU vs NPU 加速比不明显？**
A: 确认模型已转换为 QNN 格式，检查性能模式设置（Burst/Default）
```python
# 设置 Burst 模式
model.set_performance_mode("BURST")
```

#### Git 问题
**Q: 无法推送代码（push rejected）？**
A: 先拉取远程更改，解决冲突后再推送
```bash
git pull --rebase origin main
# 解决冲突后
git push origin main
```

**Q: 时间到了还没推送完成怎么办？**
A: 使用快速保存命令（10 秒内完成）
```bash
git add . && git commit -m "WIP: 临时保存" && git push
```

#### 远程设备问题
**Q: 无法登录远程 AIPC？**
A: 检查是否在分配的时间段内，确认账号密码正确，联系小助手

**Q: 远程设备数据丢失？**
A: 组委会不负责数据找回，请务必将代码定期推送到 Git，并备份到本地电脑

**Q: 如何访问本地电脑文件？**
A: 通过远程桌面重定向功能，将本地磁盘映射到远程 AIPC
```
Windows: mstsc.exe -> 显示选项 -> 本地资源 -> 驱动器
Mac: Windows App -> 添加电脑 -> 重定向文件夹
```

## 远程 AIPC 预装模型资源

远程设备上已预装多个量化模型，可直接使用，无需自行下载转换。

### 可用模型列表

位置：`C:\model\`

| 模型名称 | 参数量 | 量化版本 | 文件大小 | 推荐用途 |
|---------|-------|---------|---------|---------|
| **Qwen2.0-7B-SSD** ⭐️ | 7B | QNN 2.34 | ~4.2GB | **推荐首选**，对话/分析，速度快 |
| llama3.1-8b | 8B | QNN 2.38 | ~4.3GB | 对话生成，英文效果好 |
| llama3.2-3b | 3B | QNN 2.37 | ~2.3GB | 轻量级场景，响应最快 |

### 推荐使用策略

#### 首选：Qwen2.0-7B-SSD ⭐️
```python
# backend/config.py
MODEL_PATH = "C:/model/Qwen2.0-7B-SSD-8380-2.34"
MODEL_NAME = "Qwen2.0-7B-SSD"

# 推荐理由：
# - 7B 参数量，性能与速度平衡
# - SSD (Speculative Decoding)，生成速度快
# - 中文支持好，适合本项目
# - QNN 2.34 版本稳定
```

#### 备选：llama3.2-3b（轻量级）
```python
# 适用场景：需要更快响应速度，内存受限
MODEL_PATH = "C:/model/llama3.2-3b-8380-qnn2.37"
MODEL_NAME = "llama3.2-3b"

# 优势：
# - 3B 参数量，内存占用小 (~2.3GB)
# - 推理速度最快（< 300ms）
# - 适合简单对话和分类任务
```

#### 备选：llama3.1-8b（大模型）
```python
# 适用场景：需要更强推理能力，不追求极致速度
MODEL_PATH = "C:/model/llama3.1-8b-8380-qnn2.38"
MODEL_NAME = "llama3.1-8b"

# 优势：
# - 8B 参数量，推理能力强
# - 最新 QNN 2.38 版本
# - 英文效果好，适合国际化场景
```

### 模型加载代码示例

```python
# backend/models/model_loader.py
import os
from qai_appbuilder import QNNContext, QNNConfig

def load_model_if_needed():
    """加载预装的 NPU 模型"""

    # 1. 配置模型路径（使用预装模型）
    MODEL_PATH = "C:/model/Qwen2.0-7B-SSD-8380-2.34"

    # 2. 验证模型文件存在
    if not os.path.exists(MODEL_PATH):
        raise FileNotFoundError(f"模型路径不存在: {MODEL_PATH}")

    # 3. 配置 QNN
    config = QNNConfig(
        backend="HTP",  # 使用 NPU (Hexagon Tensor Processor)
        log_level="INFO",
        performance_mode="BURST"  # 高性能模式
    )

    # 4. 加载模型
    model = QNNContext(
        model_name="Qwen2.0-7B-SSD",
        model_path=MODEL_PATH,
        config=config
    )

    print(f"✓ 模型已加载: {MODEL_PATH}")
    print(f"✓ 运行设备: NPU (Hexagon)")

    return model

# 使用示例
model = load_model_if_needed()
```

### 性能基准数据（参考）

基于远程 AIPC 实测数据：

| 模型 | 推理延迟 | CPU vs NPU 加速比 | 内存占用 | 推荐场景 |
|------|---------|------------------|---------|---------|
| **Qwen2.0-7B-SSD** | ~400ms | 4.2x | ~1.8GB | **通用推荐** |
| llama3.2-3b | ~280ms | 5.3x | ~1.2GB | 快速响应 |
| llama3.1-8b | ~520ms | 3.8x | ~2.1GB | 复杂推理 |

### 验证模型可用性

```bash
# 检查模型文件是否存在
dir C:\model

# 快速测试模型加载（在项目根目录执行）
python backend/test_model_loading.py
```

### NPU 驱动说明

远程设备已安装：`Qualcomm_Hexagon_NPU_Driver-v1.x`

- 位置：`C:\model\Qualcomm_Hexagon_NPU_Driver-v1...`
- 无需手动安装或配置
- QAI AppBuilder 会自动调用

### 注意事项

⚠️ **重要提醒**:
- ✅ 模型文件已预装，**直接使用**，无需自行下载
- ✅ 不要修改 `C:\model\` 目录下的文件
- ✅ 不要将模型文件提交到 Git（已在 .gitignore）
- ✅ 如需自定义模型，放到项目目录 `backend/models/custom/`
- ✅ 首次加载模型需要 5-10 秒初始化时间（正常现象）

### 如何在项目中使用预装模型（远程 AI 必读）⭐️

#### 1. 基本使用（Python 后端）

```python
# backend/your_file.py
from models.model_loader import load_model_if_needed

# 加载推荐模型（Qwen2.0-7B-SSD）
model = load_model_if_needed()

# 执行推理
result = model.infer(
    prompt="分析这段数据的趋势",
    max_new_tokens=128,
    temperature=0.7
)

print(f"输出: {result}")
```

#### 2. 使用指定模型

```python
from models.model_loader import NPUModelLoader

# 使用 llama3.2-3b（更快，轻量级）
loader = NPUModelLoader(model_key="llama3.2-3b")
model = loader.load()
result = loader.infer("查询内容")

# 使用 llama3.1-8b（更强推理能力）
loader = NPUModelLoader(model_key="llama3.1-8b")
model = loader.load()
```

#### 3. 验证模型工作正常

```bash
# 在 backend 目录运行测试
cd backend
python test_model_loading.py

# 应该看到：
# ✓ 模型加载成功
# ✓ NPU推理延迟: XXXms
# ✓ 所有测试通过
```

#### 4. API 接口使用

```bash
# 启动后端服务
cd backend
python main.py

# 测试接口
# 1. 列出可用模型
curl http://localhost:8000/api/npu/models

# 2. 性能基准测试
curl http://localhost:8000/api/npu/benchmark

# 3. 数据分析
curl -X POST "http://localhost:8000/api/npu/analyze" \
  -H "Content-Type: application/json" \
  -d '{"query":"分析数据趋势","max_tokens":128}'
```

#### 5. 前端集成示例

```typescript
// src/services/npuService.ts
import axios from 'axios';

export const analyzeData = async (query: string) => {
  const response = await axios.post('/api/npu/analyze', {
    query,
    max_tokens: 128
  });

  return response.data; // 返回四色卡片和性能数据
};
```

#### 6. 常见问题排查

**问题 1: 模型加载失败**
```bash
# 检查模型文件是否存在
dir C:\model\Qwen2.0-7B-SSD-8380-2.34

# 如果不存在，解压 .zip 文件
cd C:\model
powershell Expand-Archive -Path "Qwen2.0-7B-SSD-8380-2.34.zip" -DestinationPath "."
```

**问题 2: 推理延迟超过 500ms**
```python
# backend/models/model_loader.py
# 确保使用 BURST 模式
QNN_CONFIG = {
    "performance_mode": "BURST"  # 高性能模式
}

# 或减少生成 token 数
result = loader.infer(prompt, max_new_tokens=64)  # 从 512 减到 64
```

**问题 3: 内存不足**
```python
# 使用更小的模型
loader = NPUModelLoader(model_key="llama3.2-3b")  # 3B 参数，内存占用更少
```

#### 7. 完整集成文档

**详细步骤请阅读**: `docs/NPU_MODEL_INTEGRATION.md`

该文档包含：
- ✅ Step-by-step 集成指南
- ✅ 完整的代码示例
- ✅ 性能优化建议
- ✅ 常见错误排查
- ✅ 验证清单

**关键文件**：
- `backend/models/model_loader.py` - 模型加载器（核心）
- `backend/test_model_loading.py` - 测试脚本
- `backend/routes/npu_routes.py` - API 接口
- `docs/NPU_MODEL_INTEGRATION.md` - 详细文档

#### 8. 开发流程（远程 AI 必须遵守）

**每次开始工作**：
```bash
# 1. 拉取最新代码
git pull origin main

# 2. 验证模型可用
cd backend
python -c "from models.model_loader import load_model_if_needed; load_model_if_needed()"

# 3. 开始开发
```

**开发期间**：
```bash
# 每完成一个功能，测试 NPU 性能
python test_model_loading.py

# 确保推理延迟 < 500ms
```

**工作结束前**：
```bash
# 提交代码（包含性能数据）
git add .
git commit -m "feat: 集成 NPU 模型

工作时段: 2026-01-XX XX:XX-XX:XX
完成内容:
- 集成 Qwen2.0-7B-SSD 模型
- NPU 推理延迟: XXXms
- API 接口测试通过

性能数据:
- 推理延迟: XXXms
- CPU vs NPU 加速比: X.Xx

Co-Authored-By: Claude Sonnet 4.5 <noreply@anthropic.com>"

git push origin main
```

#### 9. 禁止行为（严格执行）⚠️

- ❌ **不要修改** `C:\model\` 目录下的文件
- ❌ **不要尝试**自己转换模型（已经转换好了）
- ❌ **不要提交**模型文件到 Git（太大，没必要）
- ❌ **不要跳过**性能测试（必须验证 < 500ms）
- ❌ **不要使用** CPU 推理（必须使用 NPU）

## 技术支持

### 官方答疑渠道（推荐）⭐️

#### 1. 高通开发者论坛（主要渠道）
- **地址**: https://bbs.csdn.net/forums/qualcomm?typeId=9305416
- **使用流程**:
  1. Step1: 进入【AI 大赛】专区发帖
  2. Step2: 将问题链接发到微信群或私发小助手
  3. Step3: 专人解答（工作时间 9:00-18:00）

#### 2. AIPC 赛道答疑文档
- **地址**: https://docs.qq.com/sheet/DVVh4WkRDUmRTcWFZ
- 在线协作文档，可查看其他团队的问题与解答
- 若在论坛已提问，可在此文档注明链接

#### 3. 高频问答合集
- **地址**: https://doc.weixin.qq.com/sheet/e3_AQ0ACwY5AGoCNcUcN17rjTEeXpO8S
- 包含历次直播问答和常见问题汇总

### 技术文档资源

#### QAI AppBuilder 文档
- **本地路径**: `资料参考/ai-engine-direct-helper/`
- **官方文档**: https://docs.qualcomm.com/bundle/publicresource/topics/80-63442-50/introduction.html
- **CLI 工具文档**: https://docs.qualcomm.com/bundle/publicresource/topics/80-63442-50/tools.html
- **C API 文档**: https://docs.qualcomm.com/bundle/publicresource/topics/80-63442-50/api.html

#### 高通 AI 工具链
- **Qualcomm AI Stack 文档**: https://docs.qualcomm.com/bundle/publicresource/topics/80-63442-50/introduction.html
- **QNN SDK 文档**: https://docs.qualcomm.com/bundle/publicresource/topics/80-63442-10/htp_backend.html
- **QAIRT Visualizer**: https://docs.qualcomm.com/bundle/publicresource/topics/80-87189-1/overview.html

#### 培训视频回放
- 第一期：终端侧 AI 技术展望 & 高通工具链介绍
- 第二期：QAI AppBuilder 开发指南
- 第三期：QAI AppBuilder 移动端开发
- 第四期：AI 模型开发部署工具包
- 第五期：示例代码解读与应用演示
- 第六期：开发实战示例

**观看地址**: https://live.bilibili.com/3344545（往期回放）

### 模型资源

#### 高通官方模型库
- **Hugging Face**: https://huggingface.co/qualcomm
- **模型广场**: https://www.aidevhome.com/data/models
- 已优化并量化的模型，可直接部署

### 项目相关

- **项目 GitHub**: https://github.com/anbeime/antinet
- **提交平台**: https://qc-ai-challenge.cvmart.net/2025

### 应急联系

- 赛事小助手微信（已加入复赛群）
- 技术问题优先通过论坛提问，紧急情况联系小助手

---

## 总结：成功标准

记住：我们的目标是打造一个真正利用骁龙 NPU 能力、符合端侧隐私保护要求的智能数据工作站。

### 核心检查清单

**每次开发时都要问自己三个问题**:
1. ✅ **这个功能是否需要 NPU？** - 模型推理必须在 NPU 上运行
2. ✅ **数据是否留在本地？** - 所有数据处理本地完成，不出域
3. ✅ **性能是否达标？** - 推理延迟 < 500ms，加速比 > 2x

### 远程工作流程

**每次登录 AIPC**:
1. ✅ 第一件事：`git pull` 同步代码
2. ✅ 验证环境：运行 NPU 性能测试
3. ✅ 开始开发：专注核心功能

**工作期间**:
1. ✅ 每 2 小时推送一次代码
2. ✅ 功能完成后立即提交推送
3. ✅ 记录性能数据和问题

**时间段结束前 15 分钟**:
1. ✅ 提交所有代码（详细的提交信息）
2. ✅ 推送到远程仓库（验证成功）
3. ✅ 记录工作内容和遗留问题
4. ✅ 退出登录

### 提交前最终检查

**演示视频（≤3 分钟）**:
- ✅ 展示真实运行（非 PPT 截图）
- ✅ 核心功能完整演示
- ✅ NPU 性能数据可视化

**PPT 概述**:
- ✅ 项目背景与创新点
- ✅ 技术架构图（前端->后端->NPU）
- ✅ **算力单元说明**（CPU/GPU/NPU 分工）
- ✅ **算力选择理由**（为什么用 NPU？）
- ✅ **性能对比数据**（NPU vs CPU）
- ✅ 异构计算说明（如有）

**代码质量**:
- ✅ TypeScript strict mode，无 any 类型
- ✅ Pydantic 数据验证完整
- ✅ 推理延迟 < 500ms
- ✅ 数据不出域验证通过
- ✅ NPU 性能监控仪表板

**提交格式**:
- ✅ 文件命名：`AIPC-通用赛-[团队名称].zip`
- ✅ 文件大小 ≤ 200MB
- ✅ 结构完整（Video/PPT/Doc/Code）
- ✅ 提前 24 小时提交（截止 2月5日 21:00）

---

**技术民主化，从端侧开始。** 🚀

祝复赛顺利，期待你们的精彩作品！
