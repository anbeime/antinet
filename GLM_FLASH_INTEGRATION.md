# GLM-4.7-Flash é›†æˆæ–¹æ¡ˆ

## ğŸ“Š æ¨¡å‹æ¦‚è¿°

**GLM-4.7-Flash** æ˜¯æ™ºè°± AI æœ€æ–°å‘å¸ƒçš„é«˜æ€§èƒ½æ¨¡å‹ï¼š
- **æ¶æ„**: 30B-A3B MoEï¼ˆæ€»å‚æ•°30Bï¼Œæ¿€æ´»å‚æ•°3Bï¼‰
- **æ€§èƒ½**: 20-30B å‚æ•°èŒƒå›´å†…æœ€å¼º
- **é€Ÿåº¦**: API è°ƒç”¨çº¦ 27 tokens/s
- **ä»·æ ¼**: **å…è´¹è°ƒç”¨**ï¼ˆæ™®é€šç”¨æˆ·å¹¶å‘é‡ä¸º1ï¼‰

---

## ğŸš€ æ–¹æ¡ˆ1: API è°ƒç”¨ï¼ˆæ¨èï¼‰

### ä¼˜åŠ¿
- **å®Œå…¨å…è´¹**
- **æ— éœ€æœ¬åœ°èµ„æº**
- **å³å¼€å³ç”¨**
- **27 tokens/s é€Ÿåº¦**
- **æ”¯æŒæ·±åº¦æ€è€ƒæ¨¡å¼**

### å®‰è£…ä¾èµ–

```bash
pip install zai-sdk
```

### è·å– API Key

1. è®¿é—®æ™ºè°±å®˜ç½‘: https://docs.bigmodel.cn
2. æ³¨å†Œå¹¶å®åéªŒè¯
3. è·å– API Key

### é›†æˆåˆ° Antinet

#### 1. æ›´æ–° requirements.txt

```bash
# æ·»åŠ åˆ° backend/requirements.txt
zai-sdk>=1.0.0
```

#### 2. åˆ›å»º GLM-4.7-Flash é€‚é…å™¨

```python
# backend/models/glm_flash_adapter.py
"""
GLM-4.7-Flash æ¨¡å‹é€‚é…å™¨
æ”¯æŒæ™ºè°± API è°ƒç”¨
"""
from zai import ZhipuAiClient
from typing import Optional, Dict, Any, Iterator
import logging

logger = logging.getLogger(__name__)


class GLMFlashAdapter:
    """GLM-4.7-Flash æ¨¡å‹é€‚é…å™¨"""
    
    def __init__(self, api_key: str):
        """
        åˆå§‹åŒ–é€‚é…å™¨
        
        Args:
            api_key: æ™ºè°± API Key
        """
        self.client = ZhipuAiClient(api_key=api_key)
        self.model = "glm-4.7-flash"
        logger.info(f"âœ“ GLM-4.7-Flash é€‚é…å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def infer(
        self,
        prompt: str,
        max_tokens: int = 2048,
        temperature: float = 0.7,
        thinking: bool = False,
        stream: bool = False
    ) -> str:
        """
        æ‰§è¡Œæ¨ç†
        
        Args:
            prompt: è¾“å…¥æç¤º
            max_tokens: æœ€å¤§è¾“å‡º tokens
            temperature: æ¸©åº¦å‚æ•°ï¼ˆ0-1ï¼‰
            thinking: æ˜¯å¦å¯ç”¨æ·±åº¦æ€è€ƒæ¨¡å¼
            stream: æ˜¯å¦æµå¼è¾“å‡º
            
        Returns:
            æ¨¡å‹è¾“å‡ºæ–‡æœ¬
        """
        messages = [{"role": "user", "content": prompt}]
        
        # æ„å»ºè¯·æ±‚å‚æ•°
        params = {
            "model": self.model,
            "messages": messages,
            "max_tokens": max_tokens,
            "temperature": temperature,
        }
        
        # å¯ç”¨æ·±åº¦æ€è€ƒæ¨¡å¼
        if thinking:
            params["thinking"] = {"type": "enabled"}
        
        # æµå¼è¾“å‡º
        if stream:
            params["stream"] = True
            return self._stream_infer(params)
        
        # éæµå¼è¾“å‡º
        try:
            response = self.client.chat.completions.create(**params)
            result = response.choices[0].message.content
            logger.info(f"âœ“ GLM-4.7-Flash æ¨ç†å®Œæˆï¼Œè¾“å‡ºé•¿åº¦: {len(result)}")
            return result
        except Exception as e:
            logger.error(f"âœ— GLM-4.7-Flash æ¨ç†å¤±è´¥: {e}")
            raise
    
    def _stream_infer(self, params: Dict[str, Any]) -> Iterator[str]:
        """
        æµå¼æ¨ç†
        
        Args:
            params: è¯·æ±‚å‚æ•°
            
        Yields:
            è¾“å‡ºæ–‡æœ¬ç‰‡æ®µ
        """
        try:
            response = self.client.chat.completions.create(**params)
            
            for chunk in response:
                # æ€è€ƒå†…å®¹
                if chunk.choices[0].delta.reasoning_content:
                    yield chunk.choices[0].delta.reasoning_content
                
                # è¾“å‡ºå†…å®¹
                if chunk.choices[0].delta.content:
                    yield chunk.choices[0].delta.content
                    
        except Exception as e:
            logger.error(f"âœ— GLM-4.7-Flash æµå¼æ¨ç†å¤±è´¥: {e}")
            raise
    
    def batch_infer(
        self,
        prompts: list[str],
        max_tokens: int = 2048,
        temperature: float = 0.7
    ) -> list[str]:
        """
        æ‰¹é‡æ¨ç†
        
        Args:
            prompts: è¾“å…¥æç¤ºåˆ—è¡¨
            max_tokens: æœ€å¤§è¾“å‡º tokens
            temperature: æ¸©åº¦å‚æ•°
            
        Returns:
            è¾“å‡ºæ–‡æœ¬åˆ—è¡¨
        """
        results = []
        for prompt in prompts:
            result = self.infer(
                prompt=prompt,
                max_tokens=max_tokens,
                temperature=temperature
            )
            results.append(result)
        
        return results
```

#### 3. æ›´æ–°é…ç½®æ–‡ä»¶

```python
# backend/config.py
from pydantic_settings import BaseSettings

class Settings(BaseSettings):
    # ... ç°æœ‰é…ç½® ...
    
    # GLM-4.7-Flash é…ç½®
    GLM_FLASH_API_KEY: str = ""  # ä»ç¯å¢ƒå˜é‡è¯»å–
    GLM_FLASH_ENABLED: bool = False  # æ˜¯å¦å¯ç”¨ GLM-4.7-Flash
    
    # æ¨¡å‹é€‰æ‹©
    USE_NPU: bool = True  # True: ä½¿ç”¨ NPU, False: ä½¿ç”¨ API
    
    class Config:
        env_file = ".env"
```

#### 4. åˆ›å»º .env æ–‡ä»¶

```bash
# backend/.env
GLM_FLASH_API_KEY=your-api-key-here
GLM_FLASH_ENABLED=true
USE_NPU=false
```

#### 5. æ›´æ–°æ¨¡å‹åŠ è½½å™¨

```python
# backend/models/model_loader.py
from backend.models.glm_flash_adapter import GLMFlashAdapter
from backend.config import settings

def get_model_loader():
    """è·å–æ¨¡å‹åŠ è½½å™¨ï¼ˆæ”¯æŒ NPU å’Œ APIï¼‰"""
    
    # å¦‚æœå¯ç”¨ GLM-4.7-Flash API
    if settings.GLM_FLASH_ENABLED and not settings.USE_NPU:
        logger.info("ä½¿ç”¨ GLM-4.7-Flash API")
        return GLMFlashAdapter(api_key=settings.GLM_FLASH_API_KEY)
    
    # å¦åˆ™ä½¿ç”¨ NPU
    logger.info("ä½¿ç”¨ NPU æ¨¡å‹")
    return NPUModelLoader()
```

#### 6. æ›´æ–° API è·¯ç”±

```python
# backend/routes/npu_routes.py
@router.post("/api/npu/infer")
async def npu_infer(request: InferRequest):
    """
    NPU/API æ¨ç†æ¥å£
    è‡ªåŠ¨é€‰æ‹© NPU æˆ– GLM-4.7-Flash API
    """
    try:
        loader = get_model_loader()
        
        # æ‰§è¡Œæ¨ç†
        result = loader.infer(
            prompt=request.prompt,
            max_tokens=request.max_tokens,
            temperature=request.temperature
        )
        
        return {
            "success": True,
            "result": result,
            "model": "glm-4.7-flash" if settings.GLM_FLASH_ENABLED else "qwen2-7b-ssd"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```

---

## ğŸ–¥ï¸ æ–¹æ¡ˆ2: æœ¬åœ°éƒ¨ç½²ï¼ˆOllamaï¼‰

### ç³»ç»Ÿè¦æ±‚
- **æ˜¾å­˜**: 24GBï¼ˆ4ä½é‡åŒ–ï¼‰
- **å†…å­˜**: 16GB+
- **Ollama**: v0.14.3+

### å®‰è£…æ­¥éª¤

#### 1. å®‰è£… Ollama é¢„è§ˆç‰ˆ

```bash
# ä¸‹è½½ Ollama v0.14.3
# https://github.com/ollama/ollama/releases/tag/v0.14.3
```

#### 2. ä¸‹è½½æ¨¡å‹

```bash
# 4ä½é‡åŒ–ç‰ˆæœ¬ï¼ˆæ¨èï¼‰
ollama pull glm-4.7-flash:latest

# 8ä½é‡åŒ–ç‰ˆæœ¬
ollama pull glm-4.7-flash:8b

# åŸå§‹16ä½ç‰ˆæœ¬
ollama pull glm-4.7-flash:16b
```

#### 3. å¯åŠ¨æœåŠ¡

```bash
ollama serve
```

#### 4. æµ‹è¯•æ¨ç†

```bash
ollama run glm-4.7-flash "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±"
```

### é›†æˆåˆ° Antinet

#### åˆ›å»º Ollama é€‚é…å™¨

```python
# backend/models/ollama_adapter.py
"""
Ollama æœ¬åœ°æ¨¡å‹é€‚é…å™¨
æ”¯æŒ GLM-4.7-Flash æœ¬åœ°éƒ¨ç½²
"""
import requests
import logging

logger = logging.getLogger(__name__)


class OllamaAdapter:
    """Ollama æœ¬åœ°æ¨¡å‹é€‚é…å™¨"""
    
    def __init__(self, base_url: str = "http://localhost:11434"):
        """
        åˆå§‹åŒ–é€‚é…å™¨
        
        Args:
            base_url: Ollama æœåŠ¡åœ°å€
        """
        self.base_url = base_url
        self.model = "glm-4.7-flash"
        logger.info(f"âœ“ Ollama é€‚é…å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def infer(
        self,
        prompt: str,
        max_tokens: int = 2048,
        temperature: float = 0.7,
        stream: bool = False
    ) -> str:
        """
        æ‰§è¡Œæ¨ç†
        
        Args:
            prompt: è¾“å…¥æç¤º
            max_tokens: æœ€å¤§è¾“å‡º tokens
            temperature: æ¸©åº¦å‚æ•°
            stream: æ˜¯å¦æµå¼è¾“å‡º
            
        Returns:
            æ¨¡å‹è¾“å‡ºæ–‡æœ¬
        """
        url = f"{self.base_url}/api/generate"
        
        payload = {
            "model": self.model,
            "prompt": prompt,
            "stream": stream,
            "options": {
                "num_predict": max_tokens,
                "temperature": temperature
            }
        }
        
        try:
            response = requests.post(url, json=payload)
            response.raise_for_status()
            
            if stream:
                return self._handle_stream(response)
            else:
                result = response.json()["response"]
                logger.info(f"âœ“ Ollama æ¨ç†å®Œæˆï¼Œè¾“å‡ºé•¿åº¦: {len(result)}")
                return result
                
        except Exception as e:
            logger.error(f"âœ— Ollama æ¨ç†å¤±è´¥: {e}")
            raise
    
    def _handle_stream(self, response):
        """å¤„ç†æµå¼å“åº”"""
        full_response = ""
        for line in response.iter_lines():
            if line:
                data = line.decode('utf-8')
                import json
                chunk = json.loads(data)
                if "response" in chunk:
                    full_response += chunk["response"]
        return full_response
```

---

## ğŸ“Š æ–¹æ¡ˆå¯¹æ¯”

| ç‰¹æ€§ | API è°ƒç”¨ | NPU éƒ¨ç½² | Ollama æœ¬åœ° |
|------|----------|----------|-------------|
| **æˆæœ¬** | å…è´¹ | ä¸€æ¬¡æ€§ç¡¬ä»¶ | ä¸€æ¬¡æ€§ç¡¬ä»¶ |
| **é€Ÿåº¦** | 27 tokens/s | ~450ms | å–å†³äºç¡¬ä»¶ |
| **éšç§** | æ•°æ®ä¸Šä¼  | å®Œå…¨æœ¬åœ° | å®Œå…¨æœ¬åœ° |
| **æ˜¾å­˜è¦æ±‚** | æ—  | æ—  | 24GB |
| **éƒ¨ç½²éš¾åº¦** | ç®€å• | ä¸­ç­‰ | ç®€å• |
| **ç¨³å®šæ€§** | ä¾èµ–ç½‘ç»œ | é«˜ | é«˜ |
| **å¹¶å‘** | 1 | æ— é™åˆ¶ | æ— é™åˆ¶ |

---

## ğŸ¯ æ¨èæ–¹æ¡ˆ

### å¼€å‘é˜¶æ®µ
**ä½¿ç”¨ GLM-4.7-Flash API**
- å…è´¹
- å¿«é€Ÿéƒ¨ç½²
- æ— éœ€ç¡¬ä»¶

### ç”Ÿäº§é˜¶æ®µ
**ä½¿ç”¨ NPU éƒ¨ç½²**
- æ•°æ®ä¸å‡ºåŸŸ
- ä½å»¶è¿Ÿ
- æ— å¹¶å‘é™åˆ¶
- ç¬¦åˆæ¯”èµ›è¦æ±‚

### æ··åˆæ–¹æ¡ˆ
**NPU + API åŒæ¨¡å¼**
- NPU å¤„ç†æ ¸å¿ƒä»»åŠ¡
- API å¤„ç†éæ•æ„Ÿä»»åŠ¡
- çµæ´»åˆ‡æ¢

---

## ğŸ”§ å®æ–½æ­¥éª¤

### ç¬¬1æ­¥: å®‰è£…ä¾èµ–
```bash
cd C:\test\antinet\backend
pip install zai-sdk
```

### ç¬¬2æ­¥: è·å– API Key
è®¿é—® https://docs.bigmodel.cn æ³¨å†Œå¹¶è·å–

### ç¬¬3æ­¥: é…ç½®ç¯å¢ƒå˜é‡
åˆ›å»º `backend/.env`:
```
GLM_FLASH_API_KEY=your-api-key-here
GLM_FLASH_ENABLED=true
USE_NPU=false
```

### ç¬¬4æ­¥: åˆ›å»ºé€‚é…å™¨
å¤åˆ¶ä¸Šé¢çš„ `glm_flash_adapter.py` ä»£ç 

### ç¬¬5æ­¥: æ›´æ–°é…ç½®
ä¿®æ”¹ `config.py` å’Œ `model_loader.py`

### ç¬¬6æ­¥: æµ‹è¯•
```bash
python -c "from models.glm_flash_adapter import GLMFlashAdapter; adapter = GLMFlashAdapter('your-api-key'); print(adapter.infer('ä½ å¥½'))"
```

---

## ğŸ“ˆ æ€§èƒ½å¯¹æ¯”

### GLM-4.7-Flash API
- **å»¶è¿Ÿ**: ~37ms (27 tokens/s)
- **åå**: 27 tokens/s
- **å¹¶å‘**: 1

### NPU (Qwen2-7B-SSD)
- **å»¶è¿Ÿ**: ~450ms
- **åå**: å–å†³äº token é•¿åº¦
- **å¹¶å‘**: æ— é™åˆ¶

### å»ºè®®
- **å®æ—¶å¯¹è¯**: ä½¿ç”¨ APIï¼ˆæ›´å¿«ï¼‰
- **æ‰¹é‡åˆ†æ**: ä½¿ç”¨ NPUï¼ˆæ›´ç¨³å®šï¼‰
- **æ•æ„Ÿæ•°æ®**: å¿…é¡»ä½¿ç”¨ NPU

---

## ğŸ“ æ·±åº¦æ€è€ƒæ¨¡å¼

GLM-4.7-Flash æ”¯æŒ**æ·±åº¦æ€è€ƒæ¨¡å¼**ï¼Œé€‚åˆå¤æ‚æ¨ç†ä»»åŠ¡ï¼š

```python
response = client.chat.completions.create(
    model="glm-4.7-flash",
    messages=[{"role": "user", "content": "åˆ†æè¿™ä¸ªæ•°æ®è¶‹åŠ¿"}],
    thinking={"type": "enabled"},  # å¯ç”¨æ·±åº¦æ€è€ƒ
    max_tokens=65536,
    temperature=1.0
)
```

### é€‚ç”¨åœºæ™¯
- ğŸ§  å¤æ‚æ•°æ®åˆ†æ
- ğŸ” é£é™©è¯†åˆ«
-  ç­–ç•¥å»ºè®®
- ğŸ“Š è¶‹åŠ¿é¢„æµ‹

---

##  æ€»ç»“

### ç«‹å³å¯ç”¨
1. å®‰è£… `zai-sdk`
2. è·å– API Key
3. é…ç½®ç¯å¢ƒå˜é‡
4. å¼€å§‹ä½¿ç”¨

### ä¼˜åŠ¿
- **å®Œå…¨å…è´¹**
- **æ€§èƒ½å¼ºå¤§**ï¼ˆ20-30B æœ€å¼ºï¼‰
- **é€Ÿåº¦å¿«**ï¼ˆ27 tokens/sï¼‰
- **æ”¯æŒæ·±åº¦æ€è€ƒ**
- **æ˜“äºé›†æˆ**

### ä¸‹ä¸€æ­¥
1. é›†æˆåˆ° Antinet 8-Agent ç³»ç»Ÿ
2. å¯¹æ¯” NPU å’Œ API æ€§èƒ½
3. å®ç°æ™ºèƒ½åˆ‡æ¢æœºåˆ¶
4. ä¼˜åŒ–æ¨ç†é€Ÿåº¦

---

**åˆ›å»ºæ—¶é—´**: 2026-01-26  
**æ¨¡å‹ç‰ˆæœ¬**: GLM-4.7-Flash  
**çŠ¶æ€**: å¯ç«‹å³éƒ¨ç½²
