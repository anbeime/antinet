# ğŸ™ï¸ Coqui TTS åŠŸèƒ½æ‰©å±•åˆ†æ

## â“ æ‚¨çš„é—®é¢˜

> Coqui TTS å¯ä»¥æ‹¿æ¥åšï¼š
> 1. å°åŠ©æ‰‹å¯¹è¯åŠŸèƒ½ï¼ˆè¯­éŸ³è¾“å‡ºï¼‰
> 2. è¯­éŸ³è½¬æ–‡å­—æ•´ç†æˆæ–‡æ¡£åŠŸèƒ½ï¼ˆè¯­éŸ³è¯†åˆ«ï¼‰

---

##  ç®€çŸ­å›ç­”

### 1. å°åŠ©æ‰‹å¯¹è¯åŠŸèƒ½ å¯ä»¥ï¼

**Coqui TTS** å®Œå…¨å¯ä»¥ç”¨äºå°åŠ©æ‰‹çš„è¯­éŸ³è¾“å‡ºï¼ˆæ–‡å­—è½¬è¯­éŸ³ï¼‰

### 2. è¯­éŸ³è½¬æ–‡å­— âŒ ä¸å¯ä»¥ï¼

**Coqui TTS** åªèƒ½åš **TTSï¼ˆæ–‡å­—â†’è¯­éŸ³ï¼‰**ï¼Œä¸èƒ½åš **ASRï¼ˆè¯­éŸ³â†’æ–‡å­—ï¼‰**

**éœ€è¦é¢å¤–çš„è¯­éŸ³è¯†åˆ«å·¥å…·**ï¼Œæ¨èï¼š
- **Whisper**ï¼ˆOpenAIï¼Œæœ¬åœ°è¿è¡Œï¼‰
- **Faster-Whisper**ï¼ˆæ›´å¿«çš„Whisperï¼‰
- **Vosk**ï¼ˆè½»é‡çº§ï¼Œå®Œå…¨ç¦»çº¿ï¼‰

---

## ğŸ” è¯¦ç»†åˆ†æ

### åŠŸèƒ½1: å°åŠ©æ‰‹å¯¹è¯åŠŸèƒ½ï¼ˆè¯­éŸ³è¾“å‡ºï¼‰

#### Coqui TTS å®Œå…¨é€‚ç”¨

**å·¥ä½œæµç¨‹**ï¼š
```
ç”¨æˆ·è¾“å…¥æ–‡å­—
    â†“
Antinet å¤„ç†ï¼ˆçŸ¥è¯†åº“æŸ¥è¯¢/NPUæ¨ç†ï¼‰
    â†“
ç”Ÿæˆå›ç­”æ–‡æœ¬
    â†“
Coqui TTS è½¬æ¢ä¸ºè¯­éŸ³ âœ…
    â†“
æ’­æ”¾è¯­éŸ³ç»™ç”¨æˆ·
```

**å®ç°ç¤ºä¾‹**ï¼š
```python
from TTS.api import TTS
from backend.services.chatService import chatService

class VoiceAssistant:
    """è¯­éŸ³åŠ©æ‰‹"""
    
    def __init__(self):
        # åˆå§‹åŒ–TTS
        self.tts = TTS(model_name="tts_models/zh-CN/baker/tacotron2-DDC-GST")
        # åˆå§‹åŒ–èŠå¤©æœåŠ¡
        self.chat_service = chatService
    
    def chat_with_voice(self, user_input: str) -> tuple:
        """
        å¸¦è¯­éŸ³çš„å¯¹è¯
        
        Args:
            user_input: ç”¨æˆ·è¾“å…¥çš„æ–‡å­—
        
        Returns:
            (å›ç­”æ–‡æœ¬, è¯­éŸ³æ–‡ä»¶è·¯å¾„)
        """
        # 1. æŸ¥è¯¢çŸ¥è¯†åº“
        response = self.chat_service.query(user_input, [])
        answer_text = response['response']
        
        # 2. è½¬æ¢ä¸ºè¯­éŸ³
        audio_path = f"output/voice_{int(time.time())}.wav"
        self.tts.tts_to_file(
            text=answer_text,
            file_path=audio_path
        )
        
        return answer_text, audio_path
    
    def speak(self, text: str) -> str:
        """
        æœ—è¯»æ–‡æœ¬
        
        Args:
            text: è¦æœ—è¯»çš„æ–‡æœ¬
        
        Returns:
            è¯­éŸ³æ–‡ä»¶è·¯å¾„
        """
        audio_path = f"output/speak_{int(time.time())}.wav"
        self.tts.tts_to_file(text=text, file_path=audio_path)
        return audio_path


# ä½¿ç”¨ç¤ºä¾‹
assistant = VoiceAssistant()

# ç”¨æˆ·é—®é—®é¢˜
text, audio = assistant.chat_with_voice("Antinetæ˜¯ä»€ä¹ˆï¼Ÿ")
print(f"å›ç­”: {text}")
print(f"è¯­éŸ³: {audio}")

# æ’­æ”¾è¯­éŸ³ï¼ˆå‰ç«¯å®ç°ï¼‰
```

**ä¼˜ç‚¹**ï¼š
- å®Œå…¨ç¦»çº¿
- éŸ³è´¨å¥½
- å“åº”å¿«
- ä¸ç°æœ‰çŸ¥è¯†åº“æ— ç¼é›†æˆ

---

### åŠŸèƒ½2: è¯­éŸ³è½¬æ–‡å­—ï¼ˆè¯­éŸ³è¯†åˆ«ï¼‰

#### âŒ Coqui TTS ä¸æ”¯æŒ

**Coqui TTS åªèƒ½åš**ï¼š
- TTSï¼ˆText-to-Speechï¼‰ï¼šæ–‡å­— â†’ è¯­éŸ³
- âŒ ASRï¼ˆAutomatic Speech Recognitionï¼‰ï¼šè¯­éŸ³ â†’ æ–‡å­—

**éœ€è¦ä½¿ç”¨è¯­éŸ³è¯†åˆ«å·¥å…·**ï¼š

---

## ğŸ¤ è¯­éŸ³è¯†åˆ«æ–¹æ¡ˆå¯¹æ¯”

### æ–¹æ¡ˆ1: Whisperï¼ˆæ¨èï¼‰â­â­â­â­â­

**ç®€ä»‹**ï¼šOpenAI å¼€æºçš„è¯­éŸ³è¯†åˆ«æ¨¡å‹ï¼Œæ”¯æŒæœ¬åœ°è¿è¡Œ

**ä¼˜ç‚¹**ï¼š
- å®Œå…¨ç¦»çº¿
- è¯†åˆ«å‡†ç¡®ç‡é«˜ï¼ˆæ¥è¿‘äººç±»æ°´å¹³ï¼‰
- æ”¯æŒå¤šè¯­è¨€ï¼ˆåŒ…æ‹¬ä¸­æ–‡ï¼‰
- æ”¯æŒå¤šç§æ¨¡å‹å¤§å°ï¼ˆtiny/base/small/medium/largeï¼‰
- å¯ä»¥åœ¨NPUä¸Šè¿è¡Œï¼ˆéœ€è¦è½¬æ¢ï¼‰

**ç¼ºç‚¹**ï¼š
-  è¾ƒå¤§æ¨¡å‹éœ€è¦è¾ƒå¤šå†…å­˜
-  é¦–æ¬¡æ¨ç†è¾ƒæ…¢

**å®‰è£…**ï¼š
```cmd
pip install openai-whisper
```

**ä½¿ç”¨ç¤ºä¾‹**ï¼š
```python
import whisper

class SpeechRecognizer:
    """è¯­éŸ³è¯†åˆ«å™¨ï¼ˆä½¿ç”¨Whisperï¼‰"""
    
    def __init__(self, model_size: str = "base"):
        """
        åˆå§‹åŒ–è¯­éŸ³è¯†åˆ«å™¨
        
        Args:
            model_size: æ¨¡å‹å¤§å°ï¼ˆtiny/base/small/medium/largeï¼‰
        """
        print(f"[Whisper] åŠ è½½æ¨¡å‹: {model_size}")
        self.model = whisper.load_model(model_size)
        print("[Whisper] æ¨¡å‹åŠ è½½å®Œæˆ")
    
    def transcribe(self, audio_path: str, language: str = "zh") -> str:
        """
        è¯­éŸ³è½¬æ–‡å­—
        
        Args:
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            language: è¯­è¨€ä»£ç ï¼ˆzh=ä¸­æ–‡, en=è‹±æ–‡ï¼‰
        
        Returns:
            è¯†åˆ«çš„æ–‡å­—
        """
        print(f"[Whisper] è¯†åˆ«éŸ³é¢‘: {audio_path}")
        
        result = self.model.transcribe(
            audio_path,
            language=language,
            task="transcribe"
        )
        
        text = result["text"]
        print(f"[Whisper] è¯†åˆ«ç»“æœ: {text}")
        return text
    
    def transcribe_with_timestamps(self, audio_path: str):
        """
        è¯­éŸ³è½¬æ–‡å­—ï¼ˆå¸¦æ—¶é—´æˆ³ï¼‰
        
        Returns:
            åŒ…å«æ—¶é—´æˆ³çš„è¯†åˆ«ç»“æœ
        """
        result = self.model.transcribe(audio_path, language="zh")
        
        segments = []
        for segment in result["segments"]:
            segments.append({
                "start": segment["start"],
                "end": segment["end"],
                "text": segment["text"]
            })
        
        return {
            "full_text": result["text"],
            "segments": segments
        }


# ä½¿ç”¨ç¤ºä¾‹
recognizer = SpeechRecognizer(model_size="base")

# è¯†åˆ«éŸ³é¢‘
text = recognizer.transcribe("recording.wav")
print(f"è¯†åˆ«ç»“æœ: {text}")

# å¸¦æ—¶é—´æˆ³çš„è¯†åˆ«
result = recognizer.transcribe_with_timestamps("recording.wav")
print(f"å®Œæ•´æ–‡æœ¬: {result['full_text']}")
for seg in result['segments']:
    print(f"[{seg['start']:.2f}s - {seg['end']:.2f}s] {seg['text']}")
```

**æ¨¡å‹å¤§å°å¯¹æ¯”**ï¼š

| æ¨¡å‹ | å¤§å° | å†…å­˜ | é€Ÿåº¦ | å‡†ç¡®ç‡ | æ¨èåœºæ™¯ |
|------|------|------|------|--------|---------|
| tiny | 39 MB | ~1 GB | â­â­â­â­â­ | â­â­â­ | å¿«é€Ÿæµ‹è¯• |
| base | 74 MB | ~1 GB | â­â­â­â­ | â­â­â­â­ | **æ¨è** |
| small | 244 MB | ~2 GB | â­â­â­ | â­â­â­â­ | å¹³è¡¡ |
| medium | 769 MB | ~5 GB | â­â­ | â­â­â­â­â­ | é«˜å‡†ç¡®ç‡ |
| large | 1550 MB | ~10 GB | â­ | â­â­â­â­â­ | æœ€é«˜å‡†ç¡®ç‡ |

---

### æ–¹æ¡ˆ2: Faster-Whisperï¼ˆæ¨èï¼‰â­â­â­â­â­

**ç®€ä»‹**ï¼šWhisper çš„ä¼˜åŒ–ç‰ˆæœ¬ï¼Œé€Ÿåº¦æå‡ 4-5 å€

**ä¼˜ç‚¹**ï¼š
- å®Œå…¨ç¦»çº¿
- é€Ÿåº¦å¿«ï¼ˆæ¯”åŸç‰ˆWhisperå¿«4-5å€ï¼‰
- å†…å­˜å ç”¨æ›´å°‘
- å‡†ç¡®ç‡ä¸Whisperç›¸åŒ
- APIå…¼å®¹Whisper

**å®‰è£…**ï¼š
```cmd
pip install faster-whisper
```

**ä½¿ç”¨ç¤ºä¾‹**ï¼š
```python
from faster_whisper import WhisperModel

class FastSpeechRecognizer:
    """å¿«é€Ÿè¯­éŸ³è¯†åˆ«å™¨ï¼ˆä½¿ç”¨Faster-Whisperï¼‰"""
    
    def __init__(self, model_size: str = "base"):
        print(f"[Faster-Whisper] åŠ è½½æ¨¡å‹: {model_size}")
        # device: "cpu", "cuda", "auto"
        self.model = WhisperModel(model_size, device="cpu", compute_type="int8")
        print("[Faster-Whisper] æ¨¡å‹åŠ è½½å®Œæˆ")
    
    def transcribe(self, audio_path: str, language: str = "zh") -> str:
        """è¯­éŸ³è½¬æ–‡å­—"""
        print(f"[Faster-Whisper] è¯†åˆ«éŸ³é¢‘: {audio_path}")
        
        segments, info = self.model.transcribe(
            audio_path,
            language=language,
            beam_size=5
        )
        
        # åˆå¹¶æ‰€æœ‰ç‰‡æ®µ
        text = " ".join([segment.text for segment in segments])
        
        print(f"[Faster-Whisper] è¯†åˆ«ç»“æœ: {text}")
        return text


# ä½¿ç”¨ç¤ºä¾‹
recognizer = FastSpeechRecognizer(model_size="base")
text = recognizer.transcribe("recording.wav")
```

---

### æ–¹æ¡ˆ3: Voskï¼ˆè½»é‡çº§ï¼‰â­â­â­â­

**ç®€ä»‹**ï¼šè½»é‡çº§ç¦»çº¿è¯­éŸ³è¯†åˆ«

**ä¼˜ç‚¹**ï¼š
- å®Œå…¨ç¦»çº¿
- éå¸¸è½»é‡ï¼ˆæ¨¡å‹50-500MBï¼‰
- é€Ÿåº¦å¿«
- æ”¯æŒå®æ—¶è¯†åˆ«

**ç¼ºç‚¹**ï¼š
-  å‡†ç¡®ç‡ç•¥ä½äºWhisper
-  éœ€è¦ä¸‹è½½è¯­è¨€æ¨¡å‹

**å®‰è£…**ï¼š
```cmd
pip install vosk
```

**ä½¿ç”¨ç¤ºä¾‹**ï¼š
```python
from vosk import Model, KaldiRecognizer
import wave
import json

class VoskRecognizer:
    """Voskè¯­éŸ³è¯†åˆ«å™¨"""
    
    def __init__(self, model_path: str = "model"):
        print(f"[Vosk] åŠ è½½æ¨¡å‹: {model_path}")
        self.model = Model(model_path)
        print("[Vosk] æ¨¡å‹åŠ è½½å®Œæˆ")
    
    def transcribe(self, audio_path: str) -> str:
        """è¯­éŸ³è½¬æ–‡å­—"""
        wf = wave.open(audio_path, "rb")
        rec = KaldiRecognizer(self.model, wf.getframerate())
        
        text_parts = []
        while True:
            data = wf.readframes(4000)
            if len(data) == 0:
                break
            if rec.AcceptWaveform(data):
                result = json.loads(rec.Result())
                text_parts.append(result.get("text", ""))
        
        # æœ€åçš„ç»“æœ
        final_result = json.loads(rec.FinalResult())
        text_parts.append(final_result.get("text", ""))
        
        full_text = " ".join(text_parts)
        return full_text
```

---

## ğŸ¯ å®Œæ•´è¯­éŸ³å¯¹è¯æ–¹æ¡ˆ

### æ–¹æ¡ˆï¼šWhisper + Coqui TTS

**å·¥ä½œæµç¨‹**ï¼š
```
ç”¨æˆ·è¯´è¯ï¼ˆå½•éŸ³ï¼‰
    â†“
Whisper è¯†åˆ« â†’ æ–‡å­— âœ…
    â†“
Antinet å¤„ç†ï¼ˆçŸ¥è¯†åº“æŸ¥è¯¢ï¼‰
    â†“
ç”Ÿæˆå›ç­”æ–‡æœ¬
    â†“
Coqui TTS è½¬æ¢ â†’ è¯­éŸ³ âœ…
    â†“
æ’­æ”¾ç»™ç”¨æˆ·
```

**å®Œæ•´å®ç°**ï¼š
```python
import whisper
from TTS.api import TTS
from backend.services.chatService import chatService
import time

class VoiceDialogueAssistant:
    """å®Œæ•´çš„è¯­éŸ³å¯¹è¯åŠ©æ‰‹"""
    
    def __init__(
        self,
        whisper_model: str = "base",
        tts_model: str = "tts_models/zh-CN/baker/tacotron2-DDC-GST"
    ):
        """
        åˆå§‹åŒ–è¯­éŸ³å¯¹è¯åŠ©æ‰‹
        
        Args:
            whisper_model: Whisperæ¨¡å‹å¤§å°
            tts_model: TTSæ¨¡å‹åç§°
        """
        print("[VoiceAssistant] åˆå§‹åŒ–ä¸­...")
        
        # åˆå§‹åŒ–è¯­éŸ³è¯†åˆ«ï¼ˆWhisperï¼‰
        print(f"[VoiceAssistant] åŠ è½½Whisperæ¨¡å‹: {whisper_model}")
        self.asr = whisper.load_model(whisper_model)
        
        # åˆå§‹åŒ–è¯­éŸ³åˆæˆï¼ˆCoqui TTSï¼‰
        print(f"[VoiceAssistant] åŠ è½½TTSæ¨¡å‹: {tts_model}")
        self.tts = TTS(model_name=tts_model)
        
        # åˆå§‹åŒ–èŠå¤©æœåŠ¡
        self.chat_service = chatService
        
        print("[VoiceAssistant] âœ“ åˆå§‹åŒ–å®Œæˆ")
    
    def process_audio_input(self, audio_path: str) -> dict:
        """
        å¤„ç†éŸ³é¢‘è¾“å…¥ï¼Œè¿”å›å®Œæ•´å¯¹è¯ç»“æœ
        
        Args:
            audio_path: ç”¨æˆ·å½•éŸ³æ–‡ä»¶è·¯å¾„
        
        Returns:
            {
                "user_text": "ç”¨æˆ·è¯´çš„è¯",
                "assistant_text": "åŠ©æ‰‹çš„å›ç­”",
                "assistant_audio": "åŠ©æ‰‹å›ç­”çš„è¯­éŸ³æ–‡ä»¶è·¯å¾„",
                "sources": "çŸ¥è¯†æ¥æº"
            }
        """
        # 1. è¯­éŸ³è¯†åˆ«ï¼ˆç”¨æˆ·è¾“å…¥ï¼‰
        print("[VoiceAssistant] è¯†åˆ«ç”¨æˆ·è¯­éŸ³...")
        result = self.asr.transcribe(audio_path, language="zh")
        user_text = result["text"]
        print(f"[VoiceAssistant] ç”¨æˆ·è¯´: {user_text}")
        
        # 2. æŸ¥è¯¢çŸ¥è¯†åº“
        print("[VoiceAssistant] æŸ¥è¯¢çŸ¥è¯†åº“...")
        response = self.chat_service.query(user_text, [])
        assistant_text = response['response']
        sources = response.get('sources', [])
        print(f"[VoiceAssistant] åŠ©æ‰‹å›ç­”: {assistant_text[:50]}...")
        
        # 3. è¯­éŸ³åˆæˆï¼ˆåŠ©æ‰‹å›ç­”ï¼‰
        print("[VoiceAssistant] ç”Ÿæˆè¯­éŸ³å›ç­”...")
        audio_output_path = f"output/assistant_{int(time.time())}.wav"
        self.tts.tts_to_file(
            text=assistant_text,
            file_path=audio_output_path
        )
        print(f"[VoiceAssistant] è¯­éŸ³å·²ä¿å­˜: {audio_output_path}")
        
        return {
            "user_text": user_text,
            "assistant_text": assistant_text,
            "assistant_audio": audio_output_path,
            "sources": sources
        }
    
    def text_to_speech(self, text: str, output_path: str = None) -> str:
        """
        æ–‡å­—è½¬è¯­éŸ³ï¼ˆå•ç‹¬ä½¿ç”¨ï¼‰
        
        Args:
            text: è¦è½¬æ¢çš„æ–‡å­—
            output_path: è¾“å‡ºè·¯å¾„ï¼ˆå¯é€‰ï¼‰
        
        Returns:
            è¯­éŸ³æ–‡ä»¶è·¯å¾„
        """
        if output_path is None:
            output_path = f"output/tts_{int(time.time())}.wav"
        
        self.tts.tts_to_file(text=text, file_path=output_path)
        return output_path
    
    def speech_to_text(self, audio_path: str) -> str:
        """
        è¯­éŸ³è½¬æ–‡å­—ï¼ˆå•ç‹¬ä½¿ç”¨ï¼‰
        
        Args:
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
        
        Returns:
            è¯†åˆ«çš„æ–‡å­—
        """
        result = self.asr.transcribe(audio_path, language="zh")
        return result["text"]


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆå§‹åŒ–åŠ©æ‰‹
    assistant = VoiceDialogueAssistant(whisper_model="base")
    
    # åœºæ™¯1: å®Œæ•´è¯­éŸ³å¯¹è¯
    print("\n=== åœºæ™¯1: è¯­éŸ³å¯¹è¯ ===")
    result = assistant.process_audio_input("user_question.wav")
    print(f"ç”¨æˆ·: {result['user_text']}")
    print(f"åŠ©æ‰‹: {result['assistant_text']}")
    print(f"è¯­éŸ³: {result['assistant_audio']}")
    
    # åœºæ™¯2: å•ç‹¬è¯­éŸ³è¯†åˆ«
    print("\n=== åœºæ™¯2: è¯­éŸ³è¯†åˆ« ===")
    text = assistant.speech_to_text("recording.wav")
    print(f"è¯†åˆ«ç»“æœ: {text}")
    
    # åœºæ™¯3: å•ç‹¬è¯­éŸ³åˆæˆ
    print("\n=== åœºæ™¯3: è¯­éŸ³åˆæˆ ===")
    audio = assistant.text_to_speech("æ¬¢è¿ä½¿ç”¨Antinetæ™ºèƒ½çŸ¥è¯†ç®¡å®¶")
    print(f"è¯­éŸ³æ–‡ä»¶: {audio}")
```

---

## ğŸ“„ è¯­éŸ³è½¬æ–‡å­—æ•´ç†æˆæ–‡æ¡£

### å®ç°æ–¹æ¡ˆ

```python
class VoiceToDocumentConverter:
    """è¯­éŸ³è½¬æ–‡æ¡£è½¬æ¢å™¨"""
    
    def __init__(self, whisper_model: str = "base"):
        self.asr = whisper.load_model(whisper_model)
    
    def convert_audio_to_document(
        self,
        audio_path: str,
        output_format: str = "markdown"
    ) -> str:
        """
        å°†éŸ³é¢‘è½¬æ¢ä¸ºæ–‡æ¡£
        
        Args:
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            output_format: è¾“å‡ºæ ¼å¼ï¼ˆmarkdown/txt/docxï¼‰
        
        Returns:
            æ–‡æ¡£æ–‡ä»¶è·¯å¾„
        """
        # 1. è¯­éŸ³è¯†åˆ«ï¼ˆå¸¦æ—¶é—´æˆ³ï¼‰
        print("[è½¬æ¢] è¯†åˆ«è¯­éŸ³...")
        result = self.asr.transcribe(audio_path, language="zh")
        
        # 2. æ•´ç†æ–‡æœ¬
        full_text = result["text"]
        segments = result["segments"]
        
        # 3. ç”Ÿæˆæ–‡æ¡£
        if output_format == "markdown":
            return self._generate_markdown(full_text, segments)
        elif output_format == "docx":
            return self._generate_docx(full_text, segments)
        else:
            return self._generate_txt(full_text)
    
    def _generate_markdown(self, full_text: str, segments: list) -> str:
        """ç”ŸæˆMarkdownæ–‡æ¡£"""
        output_path = f"output/transcript_{int(time.time())}.md"
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write("# è¯­éŸ³è½¬å½•æ–‡æ¡£\n\n")
            f.write(f"**ç”Ÿæˆæ—¶é—´**: {time.strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            f.write("## å®Œæ•´æ–‡æœ¬\n\n")
            f.write(full_text + "\n\n")
            f.write("## è¯¦ç»†æ—¶é—´æˆ³\n\n")
            
            for seg in segments:
                start = seg["start"]
                end = seg["end"]
                text = seg["text"]
                f.write(f"**[{start:.2f}s - {end:.2f}s]** {text}\n\n")
        
        return output_path
    
    def _generate_docx(self, full_text: str, segments: list) -> str:
        """ç”ŸæˆWordæ–‡æ¡£"""
        from docx import Document
        
        output_path = f"output/transcript_{int(time.time())}.docx"
        doc = Document()
        
        # æ·»åŠ æ ‡é¢˜
        doc.add_heading('è¯­éŸ³è½¬å½•æ–‡æ¡£', 0)
        doc.add_paragraph(f"ç”Ÿæˆæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}")
        
        # æ·»åŠ å®Œæ•´æ–‡æœ¬
        doc.add_heading('å®Œæ•´æ–‡æœ¬', 1)
        doc.add_paragraph(full_text)
        
        # æ·»åŠ æ—¶é—´æˆ³
        doc.add_heading('è¯¦ç»†æ—¶é—´æˆ³', 1)
        for seg in segments:
            start = seg["start"]
            end = seg["end"]
            text = seg["text"]
            p = doc.add_paragraph()
            p.add_run(f"[{start:.2f}s - {end:.2f}s] ").bold = True
            p.add_run(text)
        
        doc.save(output_path)
        return output_path


# ä½¿ç”¨ç¤ºä¾‹
converter = VoiceToDocumentConverter(whisper_model="base")

# è½¬æ¢ä¸ºMarkdown
md_path = converter.convert_audio_to_document("meeting.wav", "markdown")
print(f"Markdownæ–‡æ¡£: {md_path}")

# è½¬æ¢ä¸ºWord
docx_path = converter.convert_audio_to_document("meeting.wav", "docx")
print(f"Wordæ–‡æ¡£: {docx_path}")
```

---

## ğŸ“Š æ–¹æ¡ˆæ€»ç»“

### åŠŸèƒ½1: å°åŠ©æ‰‹å¯¹è¯ï¼ˆè¯­éŸ³è¾“å‡ºï¼‰

| ç»„ä»¶ | å·¥å…· | çŠ¶æ€ |
|------|------|------|
| è¯­éŸ³åˆæˆ | Coqui TTS | å¯ç”¨ |
| çŸ¥è¯†åº“ | Antinetç°æœ‰ | å¯ç”¨ |
| é›†æˆéš¾åº¦ | - | â­â­ ç®€å• |

### åŠŸèƒ½2: è¯­éŸ³è½¬æ–‡å­—æ•´ç†æ–‡æ¡£

| ç»„ä»¶ | å·¥å…· | çŠ¶æ€ |
|------|------|------|
| è¯­éŸ³è¯†åˆ« | Whisper/Faster-Whisper | æ¨è |
| æ–‡æ¡£ç”Ÿæˆ | Markdown/Docx | å¯ç”¨ |
| é›†æˆéš¾åº¦ | - | â­â­â­ ä¸­ç­‰ |

---

## ğŸ¯ æ¨èæ–¹æ¡ˆ

### å®Œæ•´è¯­éŸ³åŠŸèƒ½

**ç»„åˆ**: **Faster-Whisper** (è¯­éŸ³è¯†åˆ«) + **Coqui TTS** (è¯­éŸ³åˆæˆ)

**ä¼˜ç‚¹**:
- å®Œå…¨ç¦»çº¿
- é€Ÿåº¦å¿«
- å‡†ç¡®ç‡é«˜
- éŸ³è´¨å¥½

**å®‰è£…**:
```cmd
pip install faster-whisper TTS
```

---

##  æ€»ç»“

### é—®é¢˜ç­”æ¡ˆ

**1. Coqui TTS å¯ä»¥åšå°åŠ©æ‰‹å¯¹è¯åŠŸèƒ½å—ï¼Ÿ**
- å¯ä»¥ï¼ç”¨äºè¯­éŸ³è¾“å‡ºéƒ¨åˆ†
- éœ€è¦é…åˆè¯­éŸ³è¯†åˆ«ï¼ˆWhisperï¼‰å®ç°å®Œæ•´å¯¹è¯

**2. Coqui TTS å¯ä»¥åšè¯­éŸ³è½¬æ–‡å­—å—ï¼Ÿ**
- âŒ ä¸å¯ä»¥ï¼Coqui TTS åªèƒ½æ–‡å­—â†’è¯­éŸ³
- éœ€è¦ä½¿ç”¨ Whisper åšè¯­éŸ³â†’æ–‡å­—

### æ¨èæŠ€æœ¯æ ˆ

```
è¯­éŸ³å¯¹è¯ç³»ç»Ÿ:
  è¾“å…¥: Whisper (è¯­éŸ³â†’æ–‡å­—)
  å¤„ç†: Antinet (çŸ¥è¯†åº“æŸ¥è¯¢)
  è¾“å‡º: Coqui TTS (æ–‡å­—â†’è¯­éŸ³)

è¯­éŸ³è½¬æ–‡æ¡£:
  è¯†åˆ«: Whisper (è¯­éŸ³â†’æ–‡å­—)
  æ•´ç†: Python (æ–‡æœ¬å¤„ç†)
  è¾“å‡º: Markdown/Wordæ–‡æ¡£
```

**ä¸¤ä¸ªåŠŸèƒ½éƒ½å¯ä»¥å®ç°ï¼Œä¸”å®Œå…¨æœ¬åœ°åŒ–ï¼** ğŸ‰

---

*åˆ†ææŠ¥å‘Šåˆ›å»ºæ—¶é—´: 2026-01-26*  
*æ¨èæ–¹æ¡ˆ: Faster-Whisper + Coqui TTS*  
*çŠ¶æ€: å¯è¡Œ*
